{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Combine them all: LeNet5 pipeline with Trainer</font>\n",
    "\n",
    "Let's take a look at how we can build the training pipeline using the Trainer helper class and the other helper classes we've discussed before in this notebook.\n",
    "Import all the necessary classes and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyas/virtualenvs/pytorch_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-25 09:20:16.238016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740504016.251442  592288 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740504016.256057  592288 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-25 09:20:16.271008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib notebook\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as Fn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from trainer import Trainer, hooks, configuration\n",
    "\n",
    "from trainer.trainer_dataset import KenyanFood13Dataset, TransformedSubset\n",
    "from trainer.test_dataset import KenyanFood13DatasetTest\n",
    "\n",
    "from trainer.experinment_utils import get_mean_std, get_data\n",
    "from trainer.configuration import Model\n",
    "from trainer.utils import setup_system, patch_configs\n",
    "from trainer.metrics import AccuracyEstimator\n",
    "from trainer.tensorboard_visualizer import TensorBoardVisualizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## <font style=\"color:Green\">1. Get Training and Validation Data Loader</font>\n",
    "\n",
    "\n",
    "Define the data wrappers and transformations (the same way as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def PlotLoader(loader):\n",
    "#     # Plot few images\n",
    "#     plt.rcParams[\"figure.figsize\"] = (15, 9)\n",
    "#     plt.figure\n",
    "#     for images, labels in loader:\n",
    "#         for i in range(len(labels)):\n",
    "#             plt.subplot(3, 5, i+1)\n",
    "#             img = Fn.to_pil_image(images[i])\n",
    "#             plt.imshow(img)\n",
    "#             plt.gca().set_title('Target: {0}'.format(labels[i]))\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, test_loader, train_mean, train_std, classes = get_data(batch_size=15, data_root='../../../../data/Week7_project2_classification/KenyanFood13Dataset', num_workers=1)\n",
    "# print(classes)\n",
    "# PlotLoader(train_loader)\n",
    "# print(\"---------\")\n",
    "# PlotLoader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## <font style=\"color:Green\">2. Define the Model</font>\n",
    "\n",
    "Define the model (the same way as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## <font style=\"color:Green\">3. Start Experiment / Training</font>\n",
    "\n",
    "\n",
    "Define the experiment with the given model and given data. It's the same idea again: we keep the less-likely-to-change things inside the object and configure it with the things that are more likely to change.\n",
    "\n",
    "You may wonder, why do we put the specific metric and optimizer into the experiment code and not specify them as parameters. But the experiment class is just a handy way to store all the parts of your experiment in one place. If you change the loss function, or the optimizer, or the model - it seems like another experiment, right? So it deserves to be a separate class.\n",
    "\n",
    "The Trainer class inner structure is a bit more complicated compared to what we've discussed above - it is just to be able to cope with the different kinds of the tasks we will discuss in this course. We will elaborate a bit more on the Trainer inner structure in the following lectures and now take a look at how compact and self-descriptive the code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_config: configuration.SystemConfig = configuration.SystemConfig(),\n",
    "        dataset_config: configuration.DatasetConfig = configuration.DatasetConfig(),\n",
    "        dataloader_config: configuration.DataloaderConfig = configuration.DataloaderConfig(),\n",
    "        optimizer_config: configuration.OptimizerConfig = configuration.OptimizerConfig(),\n",
    "        trainer_config: configuration.TrainerConfig = configuration.TrainerConfig()\n",
    "    ):\n",
    "        self.loader_train, self.loader_test, self.train_mean, self.train_std, self.labels = get_data(\n",
    "            batch_size=dataloader_config.batch_size,\n",
    "            num_workers=dataloader_config.num_workers,\n",
    "            data_root=dataset_config.root_dir\n",
    "        )\n",
    "        \n",
    "        setup_system(system_config)\n",
    "\n",
    "        self.model = Model()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.metric_fn = AccuracyEstimator(topk=(1, ))\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=optimizer_config.learning_rate,\n",
    "            weight_decay=optimizer_config.weight_decay,\n",
    "            momentum=optimizer_config.momentum\n",
    "        )\n",
    "        self.lr_scheduler = MultiStepLR(\n",
    "            self.optimizer, milestones=optimizer_config.lr_step_milestones, gamma=optimizer_config.lr_gamma\n",
    "        )\n",
    "        self.visualizer = TensorBoardVisualizer(trainer_config.tensor_board_dir)\n",
    "\n",
    "    def run(self, trainer_config: configuration.TrainerConfig) -> dict:\n",
    "\n",
    "        device = torch.device(trainer_config.device)\n",
    "        self.model = self.model.to(device)\n",
    "        self.loss_fn = self.loss_fn.to(device)\n",
    "\n",
    "        model_trainer = Trainer(\n",
    "            model=self.model,\n",
    "            loader_train=self.loader_train,\n",
    "            loader_test=self.loader_test,\n",
    "            loss_fn=self.loss_fn,\n",
    "            metric_fn=self.metric_fn,\n",
    "            optimizer=self.optimizer,\n",
    "            lr_scheduler=self.lr_scheduler,\n",
    "            device=device,\n",
    "            data_getter=itemgetter(0),\n",
    "            target_getter=itemgetter(1),\n",
    "            stage_progress=trainer_config.progress_bar,\n",
    "            get_key_metric=itemgetter(\"top1\"),\n",
    "            visualizer=self.visualizer,\n",
    "            model_saving_frequency=trainer_config.model_saving_frequency,\n",
    "            save_dir=trainer_config.model_dir,\n",
    "            # model_name_prefix=trainer_config.trainer_name\n",
    "            model_name_prefix = trainer_config.model_name_prefix\n",
    "        )\n",
    "        \n",
    "        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_classification)\n",
    "        self.metrics = model_trainer.fit(trainer_config.epoch_num)\n",
    "        return self.metrics, self.train_mean, self.train_std, self.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''Run the experiment\n",
    "    '''\n",
    "    # patch configs depending on cuda availability\n",
    "    dataloader_config, trainer_config = patch_configs(epoch_num_to_set=5)\n",
    "    dataset_config = configuration.DatasetConfig()\n",
    "    experiment = Experiment(dataset_config=dataset_config, dataloader_config=dataloader_config)\n",
    "    results, train_mean, train_std, labels = experiment.run(trainer_config)\n",
    "\n",
    "    return results, train_mean, train_std, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyas/virtualenvs/pytorch_env/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "epoch: 0, test_top1: 12.5, train_loss: 2.57, test_loss: 2.548: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5855577588, 0.4688495398, 0.3509128392]) tensor([0.2732218802, 0.2737131417, 0.2738389075]) ['githeri' 'ugali' 'kachumbari' 'matoke' 'sukumawiki' 'bhaji' 'mandazi'\n",
      " 'kukuchoma' 'nyamachoma' 'pilau' 'chapati']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    results, train_mean, train_std, labels = main()\n",
    "    print(train_mean, train_std, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Predictions</font><a name=\"predictions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">Make Predictions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">Get Predictions on a Batch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = configuration.DatasetConfig()\n",
    "data_root = dataset_config.root_dir #'../../../../data/Week7_project2_classification/KenyanFood13Dataset'\n",
    "# print(data_root)\n",
    "trainer_config = configuration.TrainerConfig()\n",
    "# print(trainer_config.model_dir)\n",
    "# print(trainer_config.trainer_name)\n",
    "# dataset =  KenyanFood13DatasetTest(data_root, image_shape=256)\n",
    "\n",
    "# # print('Length of the dataset: {}'.format(len(dataset)))\n",
    "\n",
    "# img, img_id = dataset[5]\n",
    "# print(img.size)\n",
    "# print('Image_id: {}'.format(img_id))\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_dir, model_file_name):\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # loading the model and getting model parameters by using load_state_dict\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    return model, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, device, batch_input):\n",
    "    \n",
    "#     data = batch_input.to(device)\n",
    "    data = batch_input.to(\"cpu\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "\n",
    "    # Score to probability using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "\n",
    "    # get the max probability\n",
    "    pred_prob = prob.data.max(dim=1)[0]\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">Compulsary Preprocessing Transforms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_compulsary_transforms():\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">Common Image Transforms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_common_transforms(mean=(0.4611, 0.4359, 0.3905), std=(0.2193, 0.2150, 0.2109)):\n",
    "    preprocess = image_compulsary_transforms()\n",
    "    \n",
    "    common_transforms = transforms.Compose([\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    return common_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_prediction(model, data_root, train_mean, train_std, labels, submission_dir):\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "    \n",
    "    \n",
    "    test_dataset_trans =  KenyanFood13DatasetTest(data_root, image_shape=None, transform=image_common_transforms(train_mean, train_std))\n",
    "    \n",
    "    batch_size = 15\n",
    "    num_workers = 4\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        num_workers = 8\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        num_workers = 2\n",
    "    \n",
    "    # It is important to do model.eval() before prediction\n",
    "    model.eval()\n",
    "    \n",
    "    # Send model to cpu/cuda according to your system configuration\n",
    "#     model.to(device)\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    data_len = test_dataset_trans.__len__()\n",
    "    print(\"data_len: \", data_len)\n",
    "    \n",
    "    interval = 1 #int(data_len/batch_size)\n",
    "    classes = []\n",
    "    image_ids = []\n",
    "    for start in range(0, data_len, batch_size):\n",
    "        end = start + batch_size\n",
    "        end = min(end, data_len)\n",
    "        # print('start: {}, end: {}'.format(start, end))\n",
    "\n",
    "        trans_images = []\n",
    "        for index in range(start, end):\n",
    "            trans_image, image_id = test_dataset_trans[index]\n",
    "            # print('index: {}, img_id: {}'.format(index, img_id))\n",
    "    \n",
    "            trans_images.append(trans_image)\n",
    "            image_ids.append(image_id)\n",
    "        \n",
    "        trans_images = torch.stack(trans_images)\n",
    "        classes_index, prob = prediction(model, device, batch_input=trans_images)\n",
    "        # print(\"classes_index:\", classes_index)\n",
    "        \n",
    "        classes.extend([labels[class_index] for class_index in classes_index])\n",
    "    \n",
    "    data = {\n",
    "        'id': image_ids,\n",
    "        'class': classes\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    os.makedirs(submission_dir, exist_ok=True)\n",
    "    label_csv_path = os.path.join(submission_dir, 'output.csv')\n",
    "    df.to_csv(label_csv_path, sep=\",\", index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">Load Model and Run Inference</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()\n",
    "model_dir = trainer_config.model_dir\n",
    "model_file_name = trainer_config.model_name_prefix\n",
    "model, epoch, loss = load_model(m, model_dir, model_file_name)\n",
    "# print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../data/Week7_project2_classification/base_trainer/submissions\n",
      "data_len:  1638\n"
     ]
    }
   ],
   "source": [
    "# train_mean=torch.tensor([0.5772715211, 0.4631873667, 0.3466044068])\n",
    "# train_std =torch.tensor([0.2699360847, 0.2737641633, 0.2830057442])\n",
    "# labels = ['githeri', 'ugali', 'kachumbari', 'matoke', 'sukumawiki', 'bhaji', 'mandazi',\n",
    "#  'kukuchoma', 'nyamachoma', 'pilau', 'chapati', 'masalachips', 'mukimo']\n",
    "\n",
    "submission_dir = trainer_config.submission_dir\n",
    "print(submission_dir)\n",
    "# get_sample_prediction(model, data_root, train_mean, train_std, labels, \"./submissions/\")\n",
    "get_sample_prediction(model, data_root, train_mean, train_std, labels, submission_dir)\n",
    "\n",
    "# # PlotLoader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in a few lines of code, we got a more robust system that we had before - we have richer visualizations, a more configurable training process, and we separated the pipeline for the training from the model - so we can concentrate on the things that matter the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">References</font>\n",
    "\n",
    "You may wonder whether it is a common way of doing deep learning or we're doing overengineering here. We may assure you that this is a common way to do deep learning research in an industry - most of the companies and research groups invest in building these DL training frameworks for their projects, and some of them are even published to the open-source. To name a couple of them:\n",
    "- https://github.com/NVlabs/SPADE\n",
    "- https://github.com/pytorch/ignite\n",
    "- https://github.com/PyTorchLightning/pytorch-lightning\n",
    "- https://github.com/catalyst-team/catalyst\n",
    "- https://github.com/open-mmlab/mmdetection\n",
    "- https://github.com/fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
