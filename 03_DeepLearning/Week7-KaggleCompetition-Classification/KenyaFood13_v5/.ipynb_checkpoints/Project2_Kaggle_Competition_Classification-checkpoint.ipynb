{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Log</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods, which will be used to get training and validation data loader.\n",
    "\n",
    "You need to write a custom dataset class to load data.\n",
    "\n",
    "**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "```\n",
    "def get_data(args1, *agrs):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as Fn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "from typing import Callable, Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from operator import itemgetter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datetime\n",
    "\n",
    "from typing import Union, Callable\n",
    "from pathlib import Path\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_dataset.py\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This custom dataset class takes root directory and train flag, \n",
    "    and returns dataset training dataset if train flag is true \n",
    "    else it returns validation dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, image_shape=None, transform=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        init method of the class.\n",
    "        \n",
    "         Parameters:\n",
    "         \n",
    "         data_root (string): path of root directory.\n",
    "         \n",
    "         train (boolean): True for training dataset and False for test dataset.\n",
    "         \n",
    "         image_shape (int or tuple or list): [optional] int or tuple or list. Defaut is None. \n",
    "                                             If it is not None image will resize to the given shape.\n",
    "                                 \n",
    "         transform (method): method that will take PIL image and transform it.\n",
    "         \n",
    "        \"\"\"\n",
    "        \n",
    "        # get label to species mapping\n",
    "        label_csv_path = os.path.join(data_root, 'train_trial1.csv')\n",
    "        # label_csv_path = os.path.join(data_root, 'train.csv')\n",
    "        self.data_df = pd.read_csv(label_csv_path, delimiter=' *, *', engine='python')\n",
    "        self.classes = self.data_df.iloc[:, 1].unique()\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.image_ids = self.data_df.iloc[:, 0]\n",
    "\n",
    "        self.class_given_label = {image_id : image_class for image_id, image_class in enumerate(self.classes)}\n",
    "        self.label_given_class = {image_class : image_id for image_id, image_class in enumerate(self.classes)}\n",
    "        \n",
    "        # set image_resize attribute\n",
    "        if image_shape is not None:\n",
    "            if isinstance(image_shape, int):\n",
    "                self.image_shape = (image_shape, image_shape)\n",
    "            \n",
    "            elif isinstance(image_shape, tuple) or isinstance(image_shape, list):\n",
    "                assert len(image_shape) == 1 or len(image_shape) == 2, 'Invalid image_shape tuple size'\n",
    "                if len(image_shape) == 1:\n",
    "                    self.image_shape = (image_shape[0], image_shape[0])\n",
    "                else:\n",
    "                    self.image_shape = image_shape\n",
    "            else:\n",
    "                raise NotImplementedError \n",
    "                \n",
    "        else:\n",
    "            self.image_shape = image_shape\n",
    "            \n",
    "        # set transform attribute\n",
    "        self.transform = transform\n",
    "\n",
    "        # initialize the data dictionary\n",
    "        self.data_dict = {\n",
    "            'image_path': [],\n",
    "            'label': []\n",
    "        }\n",
    "        img_dir = os.path.join(data_root, 'images', 'images')\n",
    "\n",
    "        # print(\"self.data_df\", type(self.data_df))\n",
    "        for data in self.data_df.iterrows():\n",
    "            image_id = str(data[1]['id']) + '.jpg'\n",
    "            image_path = os.path.join(img_dir, image_id)\n",
    "            image_class = data[1]['class']\n",
    "            label = self.label_given_class[image_class]\n",
    "            self.data_dict['image_path'].append(image_path)\n",
    "            self.data_dict['label'].append(label)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        return length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data_dict['label'])\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        For given index, return images with resize and preprocessing.\n",
    "        \"\"\"\n",
    "        \n",
    "        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")\n",
    "        \n",
    "        if self.image_shape is not None:\n",
    "            image = F.resize(image, self.image_shape)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target = self.data_dict['label'][idx]\n",
    "        \n",
    "        return image, target            \n",
    "                \n",
    "        \n",
    "    def class_name(self, label):\n",
    "        \"\"\"\n",
    "        class label to common name mapping\n",
    "        \"\"\"\n",
    "        return self.class_given_label[label]\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedSubset(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]  # Get item from subset\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformation\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "**Define your configuration here.**\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/opencv-pytorch-classification-project-2/\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">System Configuration</font><a name=\"sys-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration.py\n",
    "\n",
    "base_dir = \"../../../../data/Week7_project2_classification/\"\n",
    "@dataclass\n",
    "class SystemConfig:\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = False  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)\n",
    "\n",
    "\n",
    "# ## <font style=\"color:green\">Data Configuration</font>\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    # root_dir: str = \"data\"  # dataset directory root\n",
    "    root_dir: str = os.path.join(base_dir, \"KenyanFood13Dataset\")\n",
    "    train_transforms: Iterable[Callable] = (\n",
    "        ToTensor(),\n",
    "    )  # data transformation to use during training data preparation\n",
    "    test_transforms: Iterable[Callable] = (\n",
    "        ToTensor(),\n",
    "    )  # data transformation to use during test data preparation\n",
    "\n",
    "\n",
    "# ## <font style=\"color:green\">Dataloader Configuration</font>\n",
    "\n",
    "@dataclass\n",
    "class DataloaderConfig:\n",
    "    batch_size: int = 5 #250  # amount of data to pass through the network at each forward-backward iteration\n",
    "    num_workers: int = 5  # number of concurrent processes using to prepare data\n",
    "\n",
    "\n",
    "# ## <font style=\"color:green\">Optimizer Configuration</font>\n",
    "\n",
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    learning_rate: float = 0.001  # determines the speed of network's weights update\n",
    "    momentum: float = 0.9  # used to improve vanilla SGD algorithm and provide better handling of local minimas\n",
    "    weight_decay: float = 0.0001  # amount of additional regularization on the weights values\n",
    "    lr_step_milestones: Iterable = (\n",
    "        30, 40\n",
    "    )  # at which epoches should we make a \"step\" in learning rate (i.e. decrease it in some manner)\n",
    "    lr_gamma: float = 0.1  # multiplier applied to current learning rate at each of lr_ctep_milestones\n",
    "\n",
    "\n",
    "# ## <font style=\"color:green\">Training Configuration</font>\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    trainer_name: str = \"base_trainer\"\n",
    "    model_name_prefix: str = trainer_name + \".pt\"\n",
    "    model_dir: str = os.path.join(base_dir, trainer_name, \"checkpoints\") # directory to save model states\n",
    "    tensor_board_dir: str = os.path.join(base_dir, trainer_name, \"runs\") \n",
    "    model_saving_frequency: int = 1  # frequency of model state savings per epochs\n",
    "    device: str = \"cpu\"  # device to use for training.\n",
    "    epoch_num: int = 1 #50  # number of times the whole dataset will be passed through the network\n",
    "    progress_bar: bool = False  # enable progress bar visualization during train process\n",
    "    submission_dir: str = os.path.join(base_dir, trainer_name, \"submissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_metrics.py\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class BaseMetric(ABC):\n",
    "    @abstractmethod\n",
    "    def update_value(self, output, target):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metric_value(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.val = val\n",
    "        self.sum += val * count\n",
    "        self.count += count\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric.py\n",
    "\n",
    "class AccuracyEstimator(BaseMetric):\n",
    "    def __init__(self, topk=(1, )):\n",
    "        self.topk = topk\n",
    "        self.metrics = [AverageMeter() for i in range(len(topk) + 1)]\n",
    "\n",
    "    def reset(self):\n",
    "        for i in range(len(self.metrics)):\n",
    "            self.metrics[i].reset()\n",
    "\n",
    "    def update_value(self, pred, target):\n",
    "        \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "        with torch.no_grad():\n",
    "            maxk = max(self.topk)\n",
    "            batch_size = target.size(0)\n",
    "\n",
    "            _, pred = pred.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "            for i, k in enumerate(self.topk):\n",
    "                correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "                self.metrics[i].update(correct_k.mul_(100.0 / batch_size).item())\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        metrics = {}\n",
    "        for i, k in enumerate(self.topk):\n",
    "            metrics[\"top{}\".format(k)] = self.metrics[i].avg\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "\n",
    "**Write the methods or classes to be used for training and validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>4.1. Training and Validation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooks.py\n",
    "\"\"\"Implementation of several hooks that used in a Trainer class.\"\"\"\n",
    "\n",
    "def train_hook_default(\n",
    "    model,\n",
    "    loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    data_getter=itemgetter(\"image\"),\n",
    "    target_getter=itemgetter(\"mask\"),\n",
    "    iterator_type=tqdm,\n",
    "    prefix=\"\",\n",
    "    stage_progress=False\n",
    "):\n",
    "    \"\"\" Default train loop function.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): torch model which will be train.\n",
    "        loader (torch.utils.DataLoader): dataset loader.\n",
    "        loss_fn (callable): loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer.\n",
    "        device (str): Specifies device at which samples will be uploaded.\n",
    "        data_getter (Callable): function object to extract input data from the sample prepared by dataloader.\n",
    "        target_getter (Callable): function object to extract target data from the sample prepared by dataloader.\n",
    "        iterator_type (iterator): type of the iterator.\n",
    "        prefix (string): prefix which will be add to the description string.\n",
    "        stage_progress (bool): if True then progress bar will be show.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of output metrics with keys:\n",
    "            loss: average loss.\n",
    "    \"\"\"\n",
    "    model = model.train()\n",
    "    iterator = iterator_type(loader, disable=not stage_progress, dynamic_ncols=True)\n",
    "    loss_avg = AverageMeter()\n",
    "    for i, sample in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = data_getter(sample).to(device)\n",
    "        targets = target_getter(sample).to(device)\n",
    "        predicts = model(inputs)\n",
    "        loss = loss_fn(predicts, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_avg.update(loss.item())\n",
    "        status = \"{0}[Train][{1}] Loss_avg: {2:.5}, Loss: {3:.5}, LR: {4:.5}\".format(\n",
    "            prefix, i, loss_avg.avg, loss_avg.val, optimizer.param_groups[0][\"lr\"]\n",
    "        )\n",
    "        iterator.set_description(status)\n",
    "    return {\"loss\": loss_avg.avg}\n",
    "\n",
    "def test_hook_default(\n",
    "    model,\n",
    "    loader,\n",
    "    loss_fn,\n",
    "    metric_fn,\n",
    "    device,\n",
    "    data_getter=itemgetter(\"image\"),\n",
    "    target_getter=itemgetter(\"mask\"),\n",
    "    iterator_type=tqdm,\n",
    "    prefix=\"\",\n",
    "    stage_progress=False,\n",
    "    get_key_metric=itemgetter(\"accuracy\")\n",
    "):\n",
    "    \"\"\" Default test loop function.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): torch model which will be train.\n",
    "        loader (torch.utils.DataLoader): dataset loader.\n",
    "        loss_fn (callable): loss function.\n",
    "        metric_fn (callable): evaluation metric function.\n",
    "        device (str): Specifies device at which samples will be uploaded.\n",
    "        data_getter (Callable): function object to extract input data from the sample prepared by dataloader.\n",
    "        target_getter (Callable): function object to extract target data from the sample prepared by dataloader.\n",
    "        iterator_type (iterator): type of the iterator.\n",
    "        prefix (string): prefix which will be add to the description string.\n",
    "        stage_progress (bool): if True then progress bar will be show.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of output metrics with keys:\n",
    "            metric: output metric.\n",
    "            loss: average loss.\n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "    iterator = iterator_type(loader, disable=not stage_progress, dynamic_ncols=True)\n",
    "    loss_avg = AverageMeter()\n",
    "    metric_fn.reset()\n",
    "    for i, sample in enumerate(iterator):\n",
    "        inputs = data_getter(sample).to(device)\n",
    "        targets = target_getter(sample).to(device)\n",
    "        with torch.no_grad():\n",
    "            predict = model(inputs)\n",
    "            loss = loss_fn(predict, targets)\n",
    "        loss_avg.update(loss.item())\n",
    "        predict = predict.softmax(dim=1).detach()\n",
    "        metric_fn.update_value(predict, targets)\n",
    "        status = \"{0}[Test][{1}] Loss_avg: {2:.5}\".format(prefix, i, loss_avg.avg)\n",
    "        if get_key_metric is not None:\n",
    "            status = status + \", Metric_avg: {0:.5}\".format(get_key_metric(metric_fn.get_metric_value()))\n",
    "        iterator.set_description(status)\n",
    "    output = {\"metric\": metric_fn.get_metric_value(), \"loss\": loss_avg.avg}\n",
    "    return output\n",
    "\n",
    "def end_epoch_hook_classification(iterator, epoch, output_train, output_test):\n",
    "    \"\"\" Default end_epoch_hook for classification tasks.\n",
    "    Arguments:\n",
    "        iterator (iter): iterator.\n",
    "        epoch (int): number of epoch to store.\n",
    "        output_train (dict): description of the train stage.\n",
    "        output_test (dict): description of the test stage.\n",
    "        trainer (Trainer): trainer object.\n",
    "    \"\"\"\n",
    "    if hasattr(iterator, \"set_description\"):\n",
    "        iterator.set_description(\n",
    "            \"epoch: {0}, test_top1: {1:.5}, train_loss: {2:.5}, test_loss: {3:.5}\".format(\n",
    "                epoch, output_test[\"metric\"][\"top1\"], output_train[\"loss\"], output_test[\"loss\"]\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>4.2. Visualizer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizer.py\n",
    "# from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Visualizer(ABC):\n",
    "    @abstractmethod\n",
    "    def update_charts(self, train_metric, train_loss, test_metric, test_loss, learning_rate, epoch):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_visualizer.py\n",
    "\n",
    "class TensorBoardVisualizer(Visualizer):\n",
    "    def __init__(self, tensor_board_dir): \n",
    "        self._writer = SummaryWriter(tensor_board_dir)\n",
    "\n",
    "    def update_charts(self, train_metric, train_loss, test_metric, test_loss, learning_rate, epoch):\n",
    "        if train_metric is not None:\n",
    "            for metric_key, metric_value in train_metric.items():\n",
    "                self._writer.add_scalar(\"data/train_metric:{}\".format(metric_key), metric_value, epoch)\n",
    "\n",
    "        for test_metric_key, test_metric_value in test_metric.items():\n",
    "            self._writer.add_scalar(\"data/test_metric:{}\".format(test_metric_key), test_metric_value, epoch)\n",
    "\n",
    "        if train_loss is not None:\n",
    "            self._writer.add_scalar(\"data/train_loss\", train_loss, epoch)\n",
    "        if test_loss is not None:\n",
    "            self._writer.add_scalar(\"data/test_loss\", test_loss, epoch)\n",
    "\n",
    "        self._writer.add_scalar(\"data/learning_rate\", learning_rate, epoch)\n",
    "\n",
    "    def close_tensorboard(self):\n",
    "        self._writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoardVisualizer(Visualizer):\n",
    "    def __init__(self, tensor_board_dir): \n",
    "        self._writer = SummaryWriter(tensor_board_dir)\n",
    "\n",
    "    def update_charts(self, train_metric, train_loss, test_metric, test_loss, learning_rate, epoch):\n",
    "        if train_metric is not None:\n",
    "            for metric_key, metric_value in train_metric.items():\n",
    "                self._writer.add_scalar(\"data/train_metric:{}\".format(metric_key), metric_value, epoch)\n",
    "\n",
    "        for test_metric_key, test_metric_value in test_metric.items():\n",
    "            self._writer.add_scalar(\"data/test_metric:{}\".format(test_metric_key), test_metric_value, epoch)\n",
    "\n",
    "        if train_loss is not None:\n",
    "            self._writer.add_scalar(\"data/train_loss\", train_loss, epoch)\n",
    "        if test_loss is not None:\n",
    "            self._writer.add_scalar(\"data/test_loss\", test_loss, epoch)\n",
    "\n",
    "        self._writer.add_scalar(\"data/learning_rate\", learning_rate, epoch)\n",
    "\n",
    "    def close_tensorboard(self):\n",
    "        self._writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py\n",
    "\n",
    "\"\"\"Unified class to make training pipeline for deep neural networks.\"\"\"\n",
    "class Trainer:  # pylint: disable=too-many-instance-attributes\n",
    "    \"\"\" Generic class for training loop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        torch model to train\n",
    "    loader_train : torch.utils.DataLoader\n",
    "        train dataset loader.\n",
    "    loader_test : torch.utils.DataLoader\n",
    "        test dataset loader\n",
    "    loss_fn : callable\n",
    "        loss function\n",
    "    metric_fn : callable\n",
    "        evaluation metric function\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer\n",
    "    lr_scheduler : torch.optim.LrScheduler\n",
    "        Learning Rate scheduler\n",
    "    configuration : TrainerConfiguration\n",
    "        a set of training process parameters\n",
    "    data_getter : Callable\n",
    "        function object to extract input data from the sample prepared by dataloader.\n",
    "    target_getter : Callable\n",
    "        function object to extract target data from the sample prepared by dataloader.\n",
    "    visualizer : Visualizer, optional\n",
    "        shows metrics values (various backends are possible)\n",
    "    # \"\"\"\n",
    "    def __init__( # pylint: disable=too-many-arguments\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        loader_train: torch.utils.data.DataLoader,\n",
    "        loader_test: torch.utils.data.DataLoader,\n",
    "        loss_fn: Callable,\n",
    "        metric_fn: Callable,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        lr_scheduler: Callable,\n",
    "        device: Union[torch.device, str] = \"cuda\",\n",
    "        model_saving_frequency: int = 1,\n",
    "        save_dir: Union[str, Path] = \"checkpoints\",\n",
    "        model_name_prefix: str = \"model\",\n",
    "        data_getter: Callable = itemgetter(\"image\"),\n",
    "        target_getter: Callable = itemgetter(\"target\"),\n",
    "        stage_progress: bool = True,\n",
    "        visualizer: Union[Visualizer, None] = None,\n",
    "        get_key_metric: Callable = itemgetter(\"top1\"),\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.loader_train = loader_train\n",
    "        self.loader_test = loader_test\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.model_saving_frequency = model_saving_frequency\n",
    "        self.save_dir = save_dir\n",
    "        self.model_name_prefix = model_name_prefix\n",
    "        self.stage_progress = stage_progress\n",
    "        self.data_getter = data_getter\n",
    "        self.target_getter = target_getter\n",
    "        self.hooks = {}\n",
    "        self.visualizer = visualizer\n",
    "        self.get_key_metric = get_key_metric\n",
    "        self.metrics = {\"epoch\": [], \"train_loss\": [], \"test_loss\": [], \"test_metric\": []}\n",
    "        self._register_default_hooks()\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        \"\"\" Fit model method.\n",
    "\n",
    "        Arguments:\n",
    "            epochs (int): number of epochs to train model.\n",
    "        \"\"\"\n",
    "        iterator = tqdm(range(epochs), dynamic_ncols=True)\n",
    "        for epoch in iterator:\n",
    "            output_train = self.hooks[\"train\"](\n",
    "                self.model,\n",
    "                self.loader_train,\n",
    "                self.loss_fn,\n",
    "                self.optimizer,\n",
    "                self.device,\n",
    "                prefix=\"[{}/{}]\".format(epoch, epochs),\n",
    "                stage_progress=self.stage_progress,\n",
    "                data_getter=self.data_getter,\n",
    "                target_getter=self.target_getter\n",
    "            )\n",
    "            output_test = self.hooks[\"test\"](\n",
    "                self.model,\n",
    "                self.loader_test,\n",
    "                self.loss_fn,\n",
    "                self.metric_fn,\n",
    "                self.device,\n",
    "                prefix=\"[{}/{}]\".format(epoch, epochs),\n",
    "                stage_progress=self.stage_progress,\n",
    "                data_getter=self.data_getter,\n",
    "                target_getter=self.target_getter,\n",
    "                get_key_metric=self.get_key_metric\n",
    "            )\n",
    "            if self.visualizer:\n",
    "                self.visualizer.update_charts(\n",
    "                    None, output_train['loss'], output_test['metric'], output_test['loss'],\n",
    "                    self.optimizer.param_groups[0]['lr'], epoch\n",
    "                )\n",
    "\n",
    "            self.metrics['epoch'].append(epoch)\n",
    "            self.metrics['train_loss'].append(output_train['loss'])\n",
    "            self.metrics['test_loss'].append(output_test['loss'])\n",
    "            self.metrics['test_metric'].append(output_test['metric'])\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                if isinstance(self.lr_scheduler, ReduceLROnPlateau):\n",
    "                    self.lr_scheduler.step(output_train['loss'])\n",
    "                else:\n",
    "                    self.lr_scheduler.step()\n",
    "\n",
    "            if self.hooks[\"end_epoch\"] is not None:\n",
    "                self.hooks[\"end_epoch\"](iterator, epoch, output_train, output_test)\n",
    "\n",
    "            if (epoch + 1) % self.model_saving_frequency == 0:\n",
    "                os.makedirs(self.save_dir, exist_ok=True)\n",
    "                \n",
    "                torch.save({\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'loss': output_test['loss'],\n",
    "                },\n",
    "                           os.path.join(self.save_dir, self.model_name_prefix) #str(datetime.datetime.now())\n",
    "                )\n",
    "        return self.metrics\n",
    "\n",
    "    def register_hook(self, hook_type, hook_fn):\n",
    "        \"\"\" Register hook method.\n",
    "\n",
    "        Arguments:\n",
    "            hook_type (string): hook type.\n",
    "            hook_fn (callable): hook function.\n",
    "        \"\"\"\n",
    "        self.hooks[hook_type] = hook_fn\n",
    "\n",
    "    def _register_default_hooks(self):\n",
    "        self.register_hook(\"train\", train_hook_default)\n",
    "        self.register_hook(\"test\", test_hook_default)\n",
    "        self.register_hook(\"end_epoch\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop the Interface for the Train Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "**Define your model in this section.**\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>5.1. Base Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            nn.Linear(in_features=64*52*52, out_features=1024), \n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(in_features=1024, out_features=13)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>5.2. Transfer Learning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_resnet18(transfer_learning=True, num_class=13):\n",
    "    # resnet = models.resnet18(pretrained=True)\n",
    "    resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    \n",
    "    if transfer_learning:\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    last_layer_in = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(last_layer_in, num_class)\n",
    "    \n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_and_scheduler(model):\n",
    "    # train_config = TrainingConfiguration()\n",
    "\n",
    "    init_learning_rate = 0.001 #train_config.init_learning_rate\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr = init_learning_rate,\n",
    "        momentum = 0.9\n",
    "    )\n",
    "\n",
    "    decay_rate = 0.001 #train_config.decay_rate\n",
    "\n",
    "    lmbda = lambda epoch: 1/(1 + decay_rate * epoch)\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\n",
    "    \n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m pretrained_resnet18(transfer_learning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# get optimizer and scheduler\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m optimizer, scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mget_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# training_configuration=TrainingConfiguration()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Tensorboard summary writer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# transfer_learning_sw = SummaryWriter(os.path.join(training_configuration.data_prefix, 'log_resnet18/base_model'))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#                                                                            data_augmentation=True)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# transfer_learning_sw.close()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[42], line 18\u001b[0m, in \u001b[0;36mget_optimizer_and_scheduler\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     15\u001b[0m lmbda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m epoch: \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m decay_rate \u001b[38;5;241m*\u001b[39m epoch)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Scheduler\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241m.\u001b[39mLambdaLR(optimizer, lr_lambda\u001b[38;5;241m=\u001b[39mlmbda)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer, scheduler\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "model = pretrained_resnet18(transfer_learning=True, num_class=13)\n",
    "# print(model)\n",
    "# get optimizer and scheduler\n",
    "optimizer, scheduler = get_optimizer_and_scheduler(model)\n",
    "\n",
    "\n",
    "# training_configuration=TrainingConfiguration()\n",
    "# Tensorboard summary writer\n",
    "# transfer_learning_sw = SummaryWriter(os.path.join(training_configuration.data_prefix, 'log_resnet18/base_model'))\n",
    "# transfer_learning_sw = SummaryWriter(os.path.join(\"./\", 'log_resnet18/base_model'))\n",
    "\n",
    "\n",
    "# train and validate\n",
    "# model, train_loss_exp2, train_acc_exp2, val_loss_exp2, val_acc_exp2 = main(model, \n",
    "#                                                                            optimizer,\n",
    "#                                                                            transfer_learning_sw,\n",
    "#                                                                            scheduler,\n",
    "#                                                                            data_augmentation=True)\n",
    "# transfer_learning_sw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "**Define those methods or classes, which have  not been covered in the above sections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils.py\n",
    "# # <font style=\"color:blue\">Utils</font>\n",
    "#\n",
    "# Implements helper functions.\n",
    "\n",
    "def patch_configs(epoch_num_to_set=TrainerConfig.epoch_num, batch_size_to_set=DataloaderConfig.batch_size):\n",
    "    \"\"\" Patches configs if cuda is not available\n",
    "\n",
    "    Returns:\n",
    "        returns patched dataloader_config and trainer_config\n",
    "\n",
    "    \"\"\"\n",
    "    # default experiment params\n",
    "    num_workers_to_set = DataloaderConfig.num_workers\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 1\n",
    "\n",
    "    dataloader_config = DataloaderConfig(batch_size=batch_size_to_set, num_workers=num_workers_to_set)\n",
    "    trainer_config = TrainerConfig(device=device, epoch_num=epoch_num_to_set)\n",
    "    return dataloader_config, trainer_config\n",
    "\n",
    "def setup_system(system_config: SystemConfig) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    np.random.seed(system_config.seed)\n",
    "    random.seed(system_config.seed)\n",
    "    torch.set_printoptions(precision=10)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(system_config.seed)\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_util.py\n",
    "\n",
    "def get_mean_std(dataset, batch_size=8, num_workers=4):\n",
    "    \n",
    "    # transform = image_preprocess_transforms()\n",
    "    \n",
    "    # loader = data_loader(data_root, transform)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    batch_mean = torch.zeros(3)\n",
    "    batch_mean_sqrd = torch.zeros(3)\n",
    "\n",
    "    for batch_data, _ in loader:\n",
    "        batch_mean += batch_data.mean(dim=(0, 2, 3)) # E[batch_i] \n",
    "        batch_mean_sqrd += (batch_data ** 2).mean(dim=(0, 2, 3)) #  E[batch_i**2]\n",
    "    \n",
    "    # E[dataset] = E[E[batch_1], E[batch_2], ...]\n",
    "    mean = batch_mean / len(loader)\n",
    "    \n",
    "    # var[X] = E[X**2] - E[X]**2\n",
    "    \n",
    "    # E[X**2] = E[E[batch_1**2], E[batch_2**2], ...]\n",
    "    # E[X]**2 = E[E[batch_1], E[batch_2], ...] ** 2\n",
    "    \n",
    "    var = (batch_mean_sqrd / len(loader)) - (mean ** 2)\n",
    "        \n",
    "    std = var ** 0.5\n",
    "    # print('mean: {}, std: {}'.format(mean, std))\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def get_data(batch_size, data_root='data', num_workers=1):\n",
    "    compulsary_preprocess = transforms.Compose([\n",
    "        # Resize to 32X32\n",
    "        # transforms.Resize((32, 32)),\n",
    "        # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
    "        # transforms.ToTensor(),\n",
    "        # subtract mean (0.1307) and divide by variance (0.3081).\n",
    "        # This mean and variance is calculated on training data (verify yourself)\n",
    "        # transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset =  KenyanFood13Dataset(data_root, image_shape=None, transform=compulsary_preprocess)\n",
    "    classes = dataset.get_classes()\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset)) # 80% for training\n",
    "    test_size = len(dataset) - train_size # 20% for validation\n",
    "\n",
    "    train_dataset_compulsary_prepocess, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    train_mean, train_std = get_mean_std(train_dataset_compulsary_prepocess, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    train_preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomVerticalFlip(),\n",
    "        # transforms.RandomCrop(28, padding=4),\n",
    "        # transforms.PILToTensor(),\n",
    "        # transforms.ConvertImageDtype(torch.float),\n",
    "        # transforms.RandomPerspective(distortion_scale=0.6, p=1),\n",
    "        # transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        transforms.Normalize(mean=train_mean, std=train_std)\n",
    "    ])\n",
    "\n",
    "    # Apply transformation to the subset\n",
    "    train_dataset_subset = TransformedSubset(train_dataset_compulsary_prepocess, train_preprocess)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, train_mean, train_std, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_config: SystemConfig = SystemConfig(),\n",
    "        dataset_config: DatasetConfig = DatasetConfig(),\n",
    "        dataloader_config: DataloaderConfig = DataloaderConfig(),\n",
    "        optimizer_config: OptimizerConfig = OptimizerConfig(),\n",
    "        trainer_config: TrainerConfig = TrainerConfig()\n",
    "    ):\n",
    "        self.loader_train, self.loader_test, self.train_mean, self.train_std, self.labels = get_data(\n",
    "            batch_size=dataloader_config.batch_size,\n",
    "            num_workers=dataloader_config.num_workers,\n",
    "            data_root=dataset_config.root_dir\n",
    "        )\n",
    "        \n",
    "        setup_system(system_config)\n",
    "\n",
    "        self.model = Model()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.metric_fn = AccuracyEstimator(topk=(1, ))\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=optimizer_config.learning_rate,\n",
    "            weight_decay=optimizer_config.weight_decay,\n",
    "            momentum=optimizer_config.momentum\n",
    "        )\n",
    "        self.lr_scheduler = MultiStepLR(\n",
    "            self.optimizer, milestones=optimizer_config.lr_step_milestones, gamma=optimizer_config.lr_gamma\n",
    "        )\n",
    "        self.visualizer = TensorBoardVisualizer(trainer_config.tensor_board_dir)\n",
    "\n",
    "    # def run(self, trainer_config: configuration.TrainerConfig) -> dict:\n",
    "    def run(self, trainer_config: TrainerConfig) -> dict:\n",
    "\n",
    "        device = torch.device(trainer_config.device)\n",
    "        self.model = self.model.to(device)\n",
    "        self.loss_fn = self.loss_fn.to(device)\n",
    "\n",
    "        model_trainer = Trainer(\n",
    "            model=self.model,\n",
    "            loader_train=self.loader_train,\n",
    "            loader_test=self.loader_test,\n",
    "            loss_fn=self.loss_fn,\n",
    "            metric_fn=self.metric_fn,\n",
    "            optimizer=self.optimizer,\n",
    "            lr_scheduler=self.lr_scheduler,\n",
    "            device=device,\n",
    "            data_getter=itemgetter(0),\n",
    "            target_getter=itemgetter(1),\n",
    "            stage_progress=trainer_config.progress_bar,\n",
    "            get_key_metric=itemgetter(\"top1\"),\n",
    "            visualizer=self.visualizer,\n",
    "            model_saving_frequency=trainer_config.model_saving_frequency,\n",
    "            save_dir=trainer_config.model_dir,\n",
    "            model_name_prefix=trainer_config.model_name_prefix\n",
    "        )\n",
    "        \n",
    "        # model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_classification)\n",
    "        model_trainer.register_hook(\"end_epoch\", end_epoch_hook_classification)\n",
    "        self.metrics = model_trainer.fit(trainer_config.epoch_num)\n",
    "        return self.metrics, self.train_mean, self.train_std, self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>6.2. Prediction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KenyanFood13DatasetTest(Dataset):\n",
    "    \"\"\"\n",
    "    This custom dataset class takes root directory and train flag, \n",
    "    and returns dataset training dataset if train flag is true \n",
    "    else it returns validation dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, image_shape=None, transform=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        init method of the class.\n",
    "        \n",
    "         Parameters:\n",
    "         \n",
    "         data_root (string): path of root directory.\n",
    "         \n",
    "         train (boolean): True for training dataset and False for test dataset.\n",
    "         \n",
    "         image_shape (int or tuple or list): [optional] int or tuple or list. Defaut is None. \n",
    "                                             If it is not None image will resize to the given shape.\n",
    "                                 \n",
    "         transform (method): method that will take PIL image and transform it.\n",
    "         \n",
    "        \"\"\"\n",
    "        \n",
    "        # get label to species mapping\n",
    "        # label_csv_path = os.path.join(data_root, 'test.csv')\n",
    "        label_csv_path = os.path.join(data_root, 'test_trial1.csv')\n",
    "        self.data_df = pd.read_csv(label_csv_path, delimiter=' *, *', engine='python')\n",
    "        self.image_ids = self.data_df.iloc[:, 0]\n",
    "        \n",
    "        # set image_resize attribute\n",
    "        if image_shape is not None:\n",
    "            if isinstance(image_shape, int):\n",
    "                self.image_shape = (image_shape, image_shape)\n",
    "            \n",
    "            elif isinstance(image_shape, tuple) or isinstance(image_shape, list):\n",
    "                assert len(image_shape) == 1 or len(image_shape) == 2, 'Invalid image_shape tuple size'\n",
    "                if len(image_shape) == 1:\n",
    "                    self.image_shape = (image_shape[0], image_shape[0])\n",
    "                else:\n",
    "                    self.image_shape = image_shape\n",
    "            else:\n",
    "                raise NotImplementedError \n",
    "                \n",
    "        else:\n",
    "            self.image_shape = image_shape\n",
    "            \n",
    "        # set transform attribute\n",
    "        self.transform = transform\n",
    "\n",
    "        # initialize the data dictionary\n",
    "        self.data_dict = {\n",
    "            'image_path': [],\n",
    "        }\n",
    "        img_dir = os.path.join(data_root, 'images', 'images')\n",
    "\n",
    "        for data in self.data_df.iterrows():\n",
    "            image_id = str(data[1]['id']) + '.jpg'\n",
    "            image_path = os.path.join(img_dir, image_id)\n",
    "            self.data_dict['image_path'].append(image_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        return length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data_dict['image_path'])\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        For given index, return images with resize and preprocessing.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")\n",
    "        \n",
    "        if self.image_shape is not None:\n",
    "            image = Fn.resize(image, self.image_shape)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, self.image_ids.iat[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_dir, model_file_name):\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # loading the model and getting model parameters by using load_state_dict\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    return model, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, device, batch_input):\n",
    "    \n",
    "#     data = batch_input.to(device)\n",
    "    data = batch_input.to(\"cpu\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "\n",
    "    # Score to probability using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "\n",
    "    # get the max probability\n",
    "    pred_prob = prob.data.max(dim=1)[0]\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_compulsary_transforms():\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_common_transforms(mean=(0.4611, 0.4359, 0.3905), std=(0.2193, 0.2150, 0.2109)):\n",
    "    preprocess = image_compulsary_transforms()\n",
    "    \n",
    "    common_transforms = transforms.Compose([\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    return common_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_prediction(model, data_root, train_mean, train_std, labels, submission_dir):\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "    \n",
    "    \n",
    "    test_dataset_trans =  KenyanFood13DatasetTest(data_root, image_shape=None, transform=image_common_transforms(train_mean, train_std))\n",
    "    \n",
    "    batch_size = 15\n",
    "    num_workers = 4\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        num_workers = 8\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        num_workers = 2\n",
    "    \n",
    "    # It is important to do model.eval() before prediction\n",
    "    model.eval()\n",
    "    \n",
    "    # Send model to cpu/cuda according to your system configuration\n",
    "#     model.to(device)\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    data_len = test_dataset_trans.__len__()\n",
    "    print(\"data_len: \", data_len)\n",
    "    \n",
    "    interval = 1 #int(data_len/batch_size)\n",
    "    classes = []\n",
    "    image_ids = []\n",
    "    for start in range(0, data_len, batch_size):\n",
    "        end = start + batch_size\n",
    "        end = min(end, data_len)\n",
    "        # print('start: {}, end: {}'.format(start, end))\n",
    "\n",
    "        trans_images = []\n",
    "        for index in range(start, end):\n",
    "            trans_image, image_id = test_dataset_trans[index]\n",
    "            # print('index: {}, img_id: {}'.format(index, img_id))\n",
    "    \n",
    "            trans_images.append(trans_image)\n",
    "            image_ids.append(image_id)\n",
    "        \n",
    "        trans_images = torch.stack(trans_images)\n",
    "        classes_index, prob = prediction(model, device, batch_input=trans_images)\n",
    "        # print(\"classes_index:\", classes_index)\n",
    "        \n",
    "        classes.extend([labels[class_index] for class_index in classes_index])\n",
    "    \n",
    "    data = {\n",
    "        'id': image_ids,\n",
    "        'class': classes\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    os.makedirs(submission_dir, exist_ok=True)\n",
    "    label_csv_path = os.path.join(submission_dir, 'output.csv')\n",
    "    df.to_csv(label_csv_path, sep=\",\", index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_output(train_mean, train_std, labels, dataset_config, trainer_config):\n",
    "    data_root = dataset_config.root_dir\n",
    "    m = Model()\n",
    "    model_dir = trainer_config.model_dir\n",
    "    model_file_name = trainer_config.model_name_prefix\n",
    "    model, epoch, loss = load_model(m, model_dir, model_file_name)\n",
    "    submission_dir = trainer_config.submission_dir\n",
    "    get_sample_prediction(model, data_root, train_mean, train_std, labels, submission_dir)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>7.5. Main Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''Run the experiment\n",
    "    '''\n",
    "    # patch configs depending on cuda availability\n",
    "    dataloader_config, trainer_config = patch_configs(epoch_num_to_set=5)\n",
    "\n",
    "    \n",
    "    # dataset_config = configuration.DatasetConfig()\n",
    "    dataset_config = DatasetConfig()\n",
    "    experiment = Experiment(dataset_config=dataset_config, dataloader_config=dataloader_config)\n",
    "    results, train_mean, train_std, labels = experiment.run(trainer_config)\n",
    "    \n",
    "    generate_prediction_output(train_mean, train_std, labels, dataset_config, trainer_config)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyas/virtualenvs/pytorch_env/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "epoch: 0, test_top1: 25.0, train_loss: 2.5729, test_loss: 2.5473: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_len:  20\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Log [5 Points]</font>\n",
    "\n",
    "**Share your TensorBoard scalars logs here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n",
    "\n",
    "\n",
    "<font style=\"color:red\">Note:</font> In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "**Share your Kaggle profile link  with us here to score , points in  the competition.**\n",
    "\n",
    "**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n",
    "\n",
    "\n",
    "**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
