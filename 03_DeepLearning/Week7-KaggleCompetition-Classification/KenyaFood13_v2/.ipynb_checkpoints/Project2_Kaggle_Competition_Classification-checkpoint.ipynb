{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Log</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods, which will be used to get training and validation data loader.\n",
    "\n",
    "You need to write a custom dataset class to load data.\n",
    "\n",
    "**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "```\n",
    "def get_data(args1, *agrs):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from typing import Callable, Iterable\n",
    "from dataclasses import dataclass\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from operator import itemgetter\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This custom dataset class takes root directory and train flag, \n",
    "    and returns dataset training dataset if train flag is true \n",
    "    else it returns validation dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, image_shape=None, transform=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        init method of the class.\n",
    "        \n",
    "         Parameters:\n",
    "         \n",
    "         data_root (string): path of root directory.\n",
    "         \n",
    "         image_shape (int or tuple or list): [optional] int or tuple or list. Defaut is None. \n",
    "                                             If it is not None image will resize to the given shape.\n",
    "                                 \n",
    "         transform (method): method that will take PIL image and transform it.\n",
    "         \n",
    "        \"\"\"\n",
    "        \n",
    "        # get label to species mapping\n",
    "        label_csv_path = os.path.join(data_root, 'train.csv')\n",
    "        self.data_df = pd.read_csv(label_csv_path, delimiter=' *, *', engine='python')\n",
    "        self.classes = self.data_df.iloc[:, 1].unique()\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.image_ids = self.data_df.iloc[:, 0]\n",
    "\n",
    "        self.class_given_label = {image_id : image_class for image_id, image_class in enumerate(self.classes)}\n",
    "        self.label_given_class = {image_class : image_id for image_id, image_class in enumerate(self.classes)}\n",
    "\n",
    "        \n",
    "        # set image_resize attribute\n",
    "        if image_shape is not None:\n",
    "            if isinstance(image_shape, int):\n",
    "                self.image_shape = (image_shape, image_shape)\n",
    "            \n",
    "            elif isinstance(image_shape, tuple) or isinstance(image_shape, list):\n",
    "                assert len(image_shape) == 1 or len(image_shape) == 2, 'Invalid image_shape tuple size'\n",
    "                if len(image_shape) == 1:\n",
    "                    self.image_shape = (image_shape[0], image_shape[0])\n",
    "                else:\n",
    "                    self.image_shape = image_shape\n",
    "            else:\n",
    "                raise NotImplementedError \n",
    "                \n",
    "        else:\n",
    "            self.image_shape = image_shape\n",
    "            \n",
    "        # set transform attribute\n",
    "        # self.transform = transform\n",
    "\n",
    "        # initialize the data dictionary\n",
    "        self.data_dict = {\n",
    "            'image_path': [],\n",
    "            'label': []\n",
    "        }\n",
    "        img_dir = os.path.join(data_root, 'images', 'images')\n",
    "\n",
    "        # print(\"self.data_df\", type(self.data_df))\n",
    "        for data in self.data_df.iterrows():\n",
    "            image_id = str(data[1]['id']) + '.jpg'\n",
    "            image_path = os.path.join(img_dir, image_id)\n",
    "            image_class = data[1]['class']\n",
    "            label = self.label_given_class[image_class]\n",
    "            self.data_dict['image_path'].append(image_path)\n",
    "            self.data_dict['label'].append(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        return length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data_dict['label'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        For given index, return images with resize and preprocessing.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")\n",
    "        \n",
    "        if self.image_shape is not None:\n",
    "            image = F.resize(image, self.image_shape)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            \n",
    "        target = self.data_dict['label'][idx]\n",
    "        \n",
    "        return image, target            \n",
    "                \n",
    "        \n",
    "    def class_name(self, label):\n",
    "        \"\"\"\n",
    "        class label to common name mapping\n",
    "        \"\"\"\n",
    "        return self.class_given_label[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 6536\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KenyanFood13Dataset' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m  KenyanFood13Dataset(data_root, image_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength of the dataset: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(dataset)))\n\u001b[0;32m----> 9\u001b[0m img, trgt \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, class name: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trgt, dataset\u001b[38;5;241m.\u001b[39mclass_name(trgt)))\n",
      "Cell \u001b[0;32mIn[3], line 86\u001b[0m, in \u001b[0;36mKenyanFood13Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     image \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mresize(image, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_shape)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[1;32m     90\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][idx]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KenyanFood13Dataset' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "# data root directory\n",
    "# data_root = '../resource/lib/publicdata/images/10-monkey-species'\n",
    "data_root = '../../../../data/Week7_project2_classification/KenyanFood13Dataset'\n",
    "\n",
    "dataset =  KenyanFood13Dataset(data_root, image_shape=256)\n",
    "\n",
    "print('Length of the dataset: {}'.format(len(dataset)))\n",
    "\n",
    "img, trgt = dataset[300]\n",
    "print(img.size)\n",
    "print('Label: {}, class name: {}'.format(trgt, dataset.class_name(trgt)))\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataset, batch_size=8, shuffle=False, num_workers=2):\n",
    "    # dataset = datasets.ImageFolder(root=data_root, transform=transform)\n",
    "\n",
    "    # data_subset = torch.utils.data.Subset(dataset,np.arange(0,len(dataset),1./subset_size).astype(int))\n",
    "\n",
    "    # loader = torch.utils.data.DataLoader(data_subset, \n",
    "    #                                      batch_size=batch_size,\n",
    "    #                                      num_workers=num_workers,\n",
    "    #                                      shuffle=shuffle)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyNormalization():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        # self.transform = transforms.Normalize((0., 0., 0.), (0.5, 0.5, 0.5))\n",
    "        mean, std = self.get_mean_std()\n",
    "        print(mean, std)\n",
    "        self.dataset_normalized = transforms.Normalize(mean, std)\n",
    "\n",
    "    def get_normalized_dataset(self):\n",
    "        return self.dataset_normalized\n",
    "        \n",
    "    # def __getitem__(self, index):\n",
    "    #     x, y = self.dataset[index]\n",
    "    #     x = self.transform(x)\n",
    "    #     return x, y\n",
    "\n",
    "    # def __len__(self):\n",
    "    #     return len(self.dataset)\n",
    "            \n",
    "\n",
    "    # def get_mean_std(self):\n",
    "    #     mean = [0.485, 0.456, 0.406] \n",
    "    #     std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "    #     return mean, std\n",
    "\n",
    "    def get_mean_std(self):\n",
    "        loader = data_loader(dataset=self.dataset, \n",
    "                             batch_size=8, \n",
    "                             shuffle=False, \n",
    "                             num_workers=1)\n",
    "        \n",
    "        batch_mean = torch.zeros(3)\n",
    "        batch_mean_sqrd = torch.zeros(3)\n",
    "\n",
    "        for batch_data, _ in loader:\n",
    "            batch_mean += batch_data.mean(dim=(0, 2, 3)) # E[batch_i] \n",
    "            batch_mean_sqrd += (batch_data ** 2).mean(dim=(0, 2, 3)) #  E[batch_i**2]\n",
    "        \n",
    "        # E[dataset] = E[E[batch_1], E[batch_2], ...]\n",
    "        mean = batch_mean / len(loader)\n",
    "        \n",
    "        # var[X] = E[X**2] - E[X]**2\n",
    "        \n",
    "        # E[X**2] = E[E[batch_1**2], E[batch_2**2], ...]\n",
    "        # E[X]**2 = E[E[batch_1], E[batch_2], ...] ** 2\n",
    "        \n",
    "        var = (batch_mean_sqrd / len(loader)) - (mean ** 2)\n",
    "            \n",
    "        std = var ** 0.5\n",
    "        print('mean: {}, std: {}'.format(mean, std))\n",
    "        \n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "dataset =  KenyanFood13Dataset(data_root, image_shape=None)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset)) # 80% for training\n",
    "validation_size = len(dataset) - train_size # 20% for validation\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "apply_normalization = ApplyNormalization(train_dataset)\n",
    "train_dataset_normalized = apply_normalization.get_normalized_dataset()\n",
    "\n",
    "print(\"train_dataset_normalized\", type(train_dataset_normalized))\n",
    "\n",
    "# # dataloader with dataset\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#         # validation_dataset,\n",
    "#         datasetA,\n",
    "#         batch_size=15,\n",
    "#         shuffle=True,\n",
    "#         num_workers=2\n",
    "#     )\n",
    "# # first_batch = next(iter(test_loader))\n",
    "# # print(first_batch.shape)\n",
    "# Plot few images\n",
    "# plt.rcParams[\"figure.figsize\"] = (15, 9)\n",
    "# plt.figure\n",
    "# # for images, labels in test_loader:\n",
    "# for images, labels in train_dataset_normalized:\n",
    "#     for i in range(len(labels)):\n",
    "#         plt.subplot(3, 5, i+1)\n",
    "#         img = F.to_pil_image(images[i])\n",
    "#         plt.imshow(img)\n",
    "#         plt.gca().set_title('Target: {0}'.format(labels[i]))\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "**Define your configuration here.**\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/opencv-pytorch-classification-project-2/\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "\n",
    "**Write the methods or classes to be used for training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop the Interface for the Train Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "**Define your model in this section.**\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "**Define those methods or classes, which have  not been covered in the above sections.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Log [5 Points]</font>\n",
    "\n",
    "**Share your TensorBoard scalars logs here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n",
    "\n",
    "\n",
    "<font style=\"color:red\">Note:</font> In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "**Share your Kaggle profile link  with us here to score , points in  the competition.**\n",
    "\n",
    "**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n",
    "\n",
    "\n",
    "**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
