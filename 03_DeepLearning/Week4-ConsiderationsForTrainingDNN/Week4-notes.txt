Optimizer
  - Gradient decent
  - SGD
  - Gradient decent with momentum
    - Gradient decent update: Wt = Wt-1 - alpha gt-1
    - Replace the gradient with the moving average of gradient over time
    - Use moving average: Wt = Wt-1 - alpha vt
    - Calcuate moving average vt = beeta vt-1 + (1-beeta)gt-1
    - Vt = beeta vt-1 + (1-beeta)gt-1
      v0 = 0
      v1 = beeta v0 + (1-beeta)g0 = (1-beeta)g0
      v2 = beeta v1 + (1-beeta)g1 = beeta * (1-beeta)g0 + (1-beeta)g1
      v is a combination of previous gradients
      beeta is the momentum parameter (default:0.9)
  - RMSPROP (Root mean square Prop)
    - Gradient decent update: Wt = Wt-1 - alpha gt-1
    - Make the step size inversely propotional to the magnitude of gradient.
    - Step size based on gradient: St = beeta St-1 + (1 - beeta) gt^2-1
    - Moving average of the square of gradient
    - When the gradient is large step size is small
    - RMSProp gradient update Wt = Wt-1 - alpha (gt-1) / (sqrt(st) + e)
    - Due to square of the gradient term, the osilations are damping along that direction. There is some instability at the end. This can be meticated by picking the weights for which the loss is minimum.
  - Adam(Adaptive movements) = Momentum + RMSPROP
    - Momentum term vt = (beeta1*vt-1 + (1-beeta1)*gt-1) / (1-beeta1^t)
      - Calculates the moving average of the gradient
    - RMS term St = beeta2*st-q + (1-beeta2)*gt-1^2 / (1-beeta2^t)
      - Calculates the moving average of the gradient square
    - beeta1^t and beeta2^t: are bias correction term. This is used so ensure that initally when vt and st are zero the orginal value of gt and gt^2 are used.
    - Weight update Wt = Wt-1 - alpha * vt/(sqrt(st) + e)
    - beeta1 ->0.9
    - beeta2 -> 0.999
    - e -> 10^(-8)
    - alpha -> tune it
Learning rate Decay
  - wt = wt-1 - alpha * dL/dw
  - Step decay
    - alpha = alpha0 * gamma ^ (floor(n/s))
      - alpha0 -> initial learning rate = 0.0001
      - n -> epoch
      - s -> step size = 2
      - gamma -> decay_rate = 0.5 
  - time based decay
    - alpha = alpha0 / (1 + gamma * n)
      - alpha0 -> initial learning rate = 0.0001
      - n -> epoch
      - gamma -> decay_rate = 0.5
  - exponential decay
    - alpha = alpha0 * gamma^ n
      - alpha0 -> initial learning rate = 0.0001
      - n -> epoch
      - gamma -> decay_rate = 0.5

Training pipeline
https://karpathy.github.io/2019/04/25/recipe/
Refer to TrainingPipeline.png
Optimizer -> How to update the weights based on the calculated gradients
  - Step 1: Data understanding
    - What is the size of the dataset?
    - How many images or data points are there in each class?
    - Is the data noisy?
      - Are there some classes that are noisizer than others?
    - Does it have outliers?
      - How bad are they?
    - How closely related are the classes?
    - Is the data set imbalanced?
    - Are there any duplicates?
    - Does the data have any biases?
    - How much downsampling can be performed?
  - Step 2: Data preparation
    - Cleanup data: Removing duplicates/ currupt files, fix labels etc
    - Shuffle data so that no ordering to it.
    - Split data to training and validata set. 
    - Create DB if large dataset
  - Step 3: Check training pipeline
    - Check if the experimenents are reproducibles
    - Check with simple neural network for checking the pipeline. 
    - Check if the inputs are right.
    - Monitor Loss and Auccracy
      - n class
        - losses = log(n) initially (check cross entopy loss)
        - Accuracy = 1/ n initially
    - Check with dummy model with mini batch and training loss should go down
    - Check with bogus data (example black and white)
      - Error in bogus data should be much higher than with actual data
    - Training with minibatch but accuracy with all training data
    - Make the loss depend only one output. The gradient on just one input should not be zero. This is test that dimention are not messes out
    - Swap the real model with the dummy model
  - Step 4: Train the model
    - Use a standard model (ResNet-50)
    - Use pretrained weight
    - optimizer Adam
    - No learning rate decay
    - No Augmentation: Try to overfit
    - Visualize in test batch
  - Step 5: Improve your model
    - Fix over fitting
    - More data
      - Collect
      - Augment
      - Sythesize
    - Dropout or BatchNorm
      - Dont use both together as it can lead to problems
      - Just use Batchnorm
    - L1/L2/MaxNorm Regularization
    - With Dropout use Maxnorm
    - Early stopping
    - Tune hyper parameter tunning
      - Learning rate
      - Learning rate decay type and parameters
      - If creating architecture from scratch
        - No. of conv layers
        - No. of filters per layer
        - Size of filter
        - Padding
        - Stride
      - Random search instead of grid search
    - Track your experiments
      - Spreadsheets
      - Sacred Board (https://github.com/chovanecm/sacredboard)
      - DVC https://dvc.org/
      - Weights and Biases(https://wandb.ai/site/)
      - Comet.ml (https://www.comet.com/site/)
    - Make one change at a time
    - Mutiple GPU - perform multiple seperate traning in each GPU
Bias-Variance tradeoff
  - Bias: how bad does the model fit the data
  - Variance: how well doest the model overfit the data
  - Irreducible error: happens due to imperfect, noisy training data
  - Total Error = Bisas + variance + Irreucible error
  - How to address High Bias?
    - Train Longer
    - Train a more complex model
    - Obtain more features
    - Decrease Regularization
    - New model architecture
  - How to address High Variance?
    - Obtain more data
    - Decrease number of features
    - Increase regularization
    - New model architecure
How to prevent overfitting
  - Get More data
    - Augment
    - Collect
    - Systhesize
  - Early stopping
  - Regularization
    - Modify the loss function
      y = w0 + w1*x + w2*x^2 + w3*x^3 + w4*x^4
      y = sum of j=0to4 wj x^j
      L = sum of i = 1..n(yi - sum of j = 0 ..4 wj x^j)^2 + L1 / L2
      - Reduse the loss and keep the weights small
      - L1 / Lasso: L + lamda * sum of j = 0 to 3 |wj|
      - L2 / Ridge: L + lamda * sum of j = 0 to 3 wj^2
      - L1 produces weights zero lot more than L2. It therfore prefers to completely remove unimportant features.
        - lamda: Controls regularization
        - sum of j = 0 to 3 wj^2: Norm of the weight vector
      - Max-norm regularization
        - if ||w|| > C, Wj = C * wj/||w||
    - Modify the architecure
      - Dropout
        - Training is 2 -3 x slower
        - User 10 - 100x learning rate
        - Use high momentum of 0.95 to 0.99
        - Use max-norm regularization
        - Dropout rate(p):
          - Hidden layer: 0.5 to 0.8
          - Input layer: 0.8
    - Normalize feature maps
      - Batch Normalization
initial learning rate for lr scheduler 10^-3 to 10^-4
    no regularization and no dropout
    Use Dropout or BatchNorm. Using both of them can create problems. Use just BatchNorm
    Dropout - maxNorm Regularization
    Early stopping
    Hyperparameter
    Learning rate deay, parameter
    Creating architectue from strach
    No. of conv layers
    No. of filters per layer
    Size of filter
    Padding
    Stride