{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Table of contents</font>\n",
    "\n",
    "- [LeNet5 Architecture](#lenet)\n",
    "- [Display the Network](#display)\n",
    "- [Get the Fashion-MNIST Data](#get-data)\n",
    "- [System Configuration](#sys-config)\n",
    "- [Training Configuration](#train-config)\n",
    "- [System Setup](#sys-setup)\n",
    "- [Training](#training)\n",
    "- [Validation](#validation)\n",
    "- [Main function](#main)\n",
    "- [Plot Loss](#plot-loss)\n",
    "- [Miscellaneous](#misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Convolutional Neural Network Using Batch Normalization</font>\n",
    "\n",
    "In this notebook, we  add batch norm layers to the LeNet network, and see how it affects network training and convergence.\n",
    "\n",
    "Instead of the MNIST dataset, which overfits easily, we will use the Fashion MNIST dataset.\n",
    "\n",
    "The figure below shows some samples from the Fashion MNIST dataset.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2021/01/c3-w3-fashion-mnist-sprite.jpg\" width=\"600\">\n",
    "\n",
    "There are 10 classes. Each training and testing example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "\n",
    "\n",
    "We want to classify images in this dataset, using the LeNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. LeNet Architecture with BatchNorm</font><a name=\"lenet\"></a>\n",
    "\n",
    "We have already explained the architecture for LeNet in the previous notebook.\n",
    "\n",
    "Here, we create another model called LeNetBN, adding Batch Normalization layers to the 2 convolution blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer which is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Display the Network</font><a name=\"display\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNetBN(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet_model = LeNet()\n",
    "print(lenet_model)\n",
    "lenetBN_model = LeNetBN()\n",
    "print(lenetBN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## <font style=\"color:green\">3. Get Fashion-MNIST Data</font><a name=\"get-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root='../../../../data/Fasion_MNIST/', num_workers=1):\n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # Resize to 32X32\n",
    "        transforms.Resize((32, 32)),\n",
    "        # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
    "        # This mean and variance is calculated on training data (verify for yourself)\n",
    "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. System Configuration</font><a name=\"sys-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Training Configuration</font><a name=\"train-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 1  # number of times the whole dataset will be passed through the network\n",
    "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "    data_root: str = \"../../../../data/Fasion_MNIST/\"  # folder to save MNIST data (default: data)\n",
    "    num_workers: int = 10  # number of concurrent processes used to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. System Setup</font><a name=\"sys-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Training</font><a name=\"training\"></a>\n",
    "We are familiar with the training pipeline used in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. Validation</font><a name=\"validation\"></a>\n",
    "\n",
    "After every few epochs **`validation`** is called, with the `trained model` and `test_loader` to get validation loss and accuracy.\n",
    "\n",
    "**Note:** We use `model.eval()` to enable evaluation mode of the model. This will stop calculating the running estimate of mean and variance of data. Using instead just the mean and variance computed while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            indx_target = target.clone()\n",
    "            data = data.to(train_config.device)\n",
    "\n",
    "            target = target.to(train_config.device)\n",
    "\n",
    "            output = model(data)\n",
    "            # add loss for each mini batch\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "\n",
    "            # get probability score using softmax\n",
    "            prob = F.softmax(output, dim=1)\n",
    "\n",
    "            # get the index of the max probability\n",
    "            pred = prob.data.max(dim=1)[1] \n",
    "\n",
    "            # add correct prediction count\n",
    "            count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "        # average over number of mini-batches\n",
    "        test_loss = test_loss / len(test_loader)  \n",
    "\n",
    "        # average over number of dataset\n",
    "        accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "            )\n",
    "        )\n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Main</font><a name=\"main\"></a>\n",
    "\n",
    "\n",
    "Here, we use the configuration parameters defined above and start  training. \n",
    "\n",
    "1. Set up system parameters like CPU/GPU, number of threads etc.\n",
    "1. Load the data using dataloaders.\n",
    "1. Create an instance of the LeNet model.\n",
    "1. Specify optimizer to use.\n",
    "1. Set up variables to track loss and accuracy and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 10\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        epochs_count=epoch_num_to_set,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_configuration.learning_rate\n",
    "    )\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hd/disk2/virtual_envs/pytorch_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../../../../data/Fasion_MNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../../data/Fasion_MNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../../../../data/Fasion_MNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../../../../data/Fasion_MNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../../data/Fasion_MNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../../../../data/Fasion_MNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../../../../data/Fasion_MNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../../data/Fasion_MNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../../../../data/Fasion_MNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../../../../data/Fasion_MNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../../data/Fasion_MNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../../../../data/Fasion_MNIST/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [1600/60000] Loss: 2.264623 Acc: 0.3125\n",
      "Train Epoch: 0 [3200/60000] Loss: 2.246929 Acc: 0.1875\n",
      "Train Epoch: 0 [4800/60000] Loss: 2.074335 Acc: 0.4375\n",
      "Train Epoch: 0 [6400/60000] Loss: 1.038868 Acc: 0.5625\n",
      "Train Epoch: 0 [8000/60000] Loss: 1.005782 Acc: 0.5625\n",
      "Train Epoch: 0 [9600/60000] Loss: 0.847363 Acc: 0.6875\n",
      "Train Epoch: 0 [11200/60000] Loss: 1.037962 Acc: 0.5000\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.951945 Acc: 0.6250\n",
      "Train Epoch: 0 [14400/60000] Loss: 0.827967 Acc: 0.6875\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.538210 Acc: 0.8125\n",
      "Train Epoch: 0 [17600/60000] Loss: 0.837204 Acc: 0.6875\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.750468 Acc: 0.8750\n",
      "Train Epoch: 0 [20800/60000] Loss: 0.811919 Acc: 0.7500\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.708548 Acc: 0.6875\n",
      "Train Epoch: 0 [24000/60000] Loss: 0.167667 Acc: 1.0000\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.802938 Acc: 0.6875\n",
      "Train Epoch: 0 [27200/60000] Loss: 0.891988 Acc: 0.5625\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.333052 Acc: 0.9375\n",
      "Train Epoch: 0 [30400/60000] Loss: 0.724349 Acc: 0.7500\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.706190 Acc: 0.6875\n",
      "Train Epoch: 0 [33600/60000] Loss: 0.392178 Acc: 0.8750\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.941251 Acc: 0.5625\n",
      "Train Epoch: 0 [36800/60000] Loss: 0.733420 Acc: 0.7500\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.246329 Acc: 0.9375\n",
      "Train Epoch: 0 [40000/60000] Loss: 0.433366 Acc: 0.9375\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.365854 Acc: 0.8125\n",
      "Train Epoch: 0 [43200/60000] Loss: 0.516721 Acc: 0.8125\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.543782 Acc: 0.8125\n",
      "Train Epoch: 0 [46400/60000] Loss: 0.532069 Acc: 0.6875\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.657662 Acc: 0.7500\n",
      "Train Epoch: 0 [49600/60000] Loss: 0.685710 Acc: 0.7500\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.285852 Acc: 0.8750\n",
      "Train Epoch: 0 [52800/60000] Loss: 0.554650 Acc: 0.7500\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.829511 Acc: 0.6875\n",
      "Train Epoch: 0 [56000/60000] Loss: 0.258118 Acc: 0.9375\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.304479 Acc: 0.8125\n",
      "Train Epoch: 0 [59200/60000] Loss: 0.313921 Acc: 0.9375\n",
      "Elapsed 10.58s, 10.58 s/epoch, 0.00 s/batch, ets 95.20s\n",
      "\n",
      "Test set: Average loss: 0.5180, Accuracy: 8033/10000 (80%)\n",
      "\n",
      "Train Epoch: 1 [1600/60000] Loss: 0.427322 Acc: 0.8750\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.607179 Acc: 0.7500\n",
      "Train Epoch: 1 [4800/60000] Loss: 0.427517 Acc: 0.9375\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.450527 Acc: 0.8750\n",
      "Train Epoch: 1 [8000/60000] Loss: 0.869308 Acc: 0.7500\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.306746 Acc: 0.8750\n",
      "Train Epoch: 1 [11200/60000] Loss: 0.389826 Acc: 0.8125\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.592179 Acc: 0.8125\n",
      "Train Epoch: 1 [14400/60000] Loss: 0.216231 Acc: 0.8750\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.398392 Acc: 0.8125\n",
      "Train Epoch: 1 [17600/60000] Loss: 0.561791 Acc: 0.8125\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.636025 Acc: 0.8125\n",
      "Train Epoch: 1 [20800/60000] Loss: 0.620483 Acc: 0.7500\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.355259 Acc: 0.8125\n",
      "Train Epoch: 1 [24000/60000] Loss: 0.491504 Acc: 0.7500\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.509351 Acc: 0.7500\n",
      "Train Epoch: 1 [27200/60000] Loss: 0.177754 Acc: 0.9375\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.303758 Acc: 0.8750\n",
      "Train Epoch: 1 [30400/60000] Loss: 0.722706 Acc: 0.7500\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.214018 Acc: 0.9375\n",
      "Train Epoch: 1 [33600/60000] Loss: 0.274933 Acc: 0.9375\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.287032 Acc: 0.8750\n",
      "Train Epoch: 1 [36800/60000] Loss: 0.344771 Acc: 0.9375\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.386918 Acc: 0.9375\n",
      "Train Epoch: 1 [40000/60000] Loss: 0.186559 Acc: 1.0000\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.483698 Acc: 0.8125\n",
      "Train Epoch: 1 [43200/60000] Loss: 0.500634 Acc: 0.8125\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.488836 Acc: 0.7500\n",
      "Train Epoch: 1 [46400/60000] Loss: 0.344698 Acc: 0.8750\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.536439 Acc: 0.7500\n",
      "Train Epoch: 1 [49600/60000] Loss: 0.469065 Acc: 0.8750\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.272098 Acc: 0.9375\n",
      "Train Epoch: 1 [52800/60000] Loss: 0.260326 Acc: 0.8750\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.767205 Acc: 0.7500\n",
      "Train Epoch: 1 [56000/60000] Loss: 0.252003 Acc: 0.9375\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.624625 Acc: 0.7500\n",
      "Train Epoch: 1 [59200/60000] Loss: 0.392678 Acc: 0.8125\n",
      "Elapsed 24.20s, 12.10 s/epoch, 0.00 s/batch, ets 96.78s\n",
      "\n",
      "Test set: Average loss: 0.4425, Accuracy: 8375/10000 (84%)\n",
      "\n",
      "Train Epoch: 2 [1600/60000] Loss: 0.399922 Acc: 0.8750\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.807353 Acc: 0.8750\n",
      "Train Epoch: 2 [4800/60000] Loss: 0.354079 Acc: 0.7500\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.403962 Acc: 0.8750\n",
      "Train Epoch: 2 [8000/60000] Loss: 0.256586 Acc: 0.9375\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.120375 Acc: 0.9375\n",
      "Train Epoch: 2 [11200/60000] Loss: 0.244158 Acc: 0.8125\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.298831 Acc: 0.8750\n",
      "Train Epoch: 2 [14400/60000] Loss: 0.513375 Acc: 0.7500\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.185917 Acc: 0.9375\n",
      "Train Epoch: 2 [17600/60000] Loss: 0.776634 Acc: 0.7500\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.838279 Acc: 0.6875\n",
      "Train Epoch: 2 [20800/60000] Loss: 0.768660 Acc: 0.6875\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.404460 Acc: 0.8125\n",
      "Train Epoch: 2 [24000/60000] Loss: 0.112325 Acc: 0.9375\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.329772 Acc: 0.9375\n",
      "Train Epoch: 2 [27200/60000] Loss: 0.364634 Acc: 0.8125\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.291428 Acc: 0.9375\n",
      "Train Epoch: 2 [30400/60000] Loss: 0.453946 Acc: 0.8750\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.155997 Acc: 0.9375\n",
      "Train Epoch: 2 [33600/60000] Loss: 0.493590 Acc: 0.8750\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.383858 Acc: 0.8125\n",
      "Train Epoch: 2 [36800/60000] Loss: 0.530386 Acc: 0.8125\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.287085 Acc: 0.8125\n",
      "Train Epoch: 2 [40000/60000] Loss: 0.837207 Acc: 0.7500\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.823542 Acc: 0.6875\n",
      "Train Epoch: 2 [43200/60000] Loss: 0.353783 Acc: 0.9375\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.357337 Acc: 0.8750\n",
      "Train Epoch: 2 [46400/60000] Loss: 0.494454 Acc: 0.8750\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.226857 Acc: 0.8750\n",
      "Train Epoch: 2 [49600/60000] Loss: 0.673087 Acc: 0.7500\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.227939 Acc: 0.9375\n",
      "Train Epoch: 2 [52800/60000] Loss: 0.178849 Acc: 0.9375\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.380790 Acc: 0.8125\n",
      "Train Epoch: 2 [56000/60000] Loss: 0.587013 Acc: 0.8125\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.292673 Acc: 0.9375\n",
      "Train Epoch: 2 [59200/60000] Loss: 0.238654 Acc: 0.8750\n",
      "Elapsed 37.71s, 12.57 s/epoch, 0.00 s/batch, ets 88.00s\n",
      "\n",
      "Test set: Average loss: 0.3884, Accuracy: 8601/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [1600/60000] Loss: 0.339696 Acc: 0.8125\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.119726 Acc: 0.9375\n",
      "Train Epoch: 3 [4800/60000] Loss: 0.227809 Acc: 0.8750\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.358715 Acc: 0.8750\n",
      "Train Epoch: 3 [8000/60000] Loss: 0.166548 Acc: 0.9375\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.760173 Acc: 0.6875\n",
      "Train Epoch: 3 [11200/60000] Loss: 0.148902 Acc: 1.0000\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.752094 Acc: 0.8125\n",
      "Train Epoch: 3 [14400/60000] Loss: 0.133611 Acc: 1.0000\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.100586 Acc: 0.9375\n",
      "Train Epoch: 3 [17600/60000] Loss: 0.885699 Acc: 0.6250\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.269721 Acc: 0.8750\n",
      "Train Epoch: 3 [20800/60000] Loss: 0.155955 Acc: 0.9375\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.667306 Acc: 0.8750\n",
      "Train Epoch: 3 [24000/60000] Loss: 0.278072 Acc: 0.9375\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.240540 Acc: 0.8750\n",
      "Train Epoch: 3 [27200/60000] Loss: 0.268414 Acc: 0.8750\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.205258 Acc: 0.9375\n",
      "Train Epoch: 3 [30400/60000] Loss: 0.348495 Acc: 0.8125\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.128608 Acc: 0.9375\n",
      "Train Epoch: 3 [33600/60000] Loss: 0.329246 Acc: 0.8750\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.143157 Acc: 0.8750\n",
      "Train Epoch: 3 [36800/60000] Loss: 0.462638 Acc: 0.6875\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.269103 Acc: 0.9375\n",
      "Train Epoch: 3 [40000/60000] Loss: 0.836720 Acc: 0.6250\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.285575 Acc: 0.8750\n",
      "Train Epoch: 3 [43200/60000] Loss: 0.395684 Acc: 0.7500\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.167738 Acc: 0.9375\n",
      "Train Epoch: 3 [46400/60000] Loss: 0.230567 Acc: 0.8750\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.746717 Acc: 0.7500\n",
      "Train Epoch: 3 [49600/60000] Loss: 0.241917 Acc: 0.8750\n",
      "Train Epoch: 3 [51200/60000] Loss: 1.240697 Acc: 0.6250\n",
      "Train Epoch: 3 [52800/60000] Loss: 0.436053 Acc: 0.8750\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.473398 Acc: 0.8125\n",
      "Train Epoch: 3 [56000/60000] Loss: 0.158354 Acc: 1.0000\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.261978 Acc: 0.9375\n",
      "Train Epoch: 3 [59200/60000] Loss: 0.360158 Acc: 0.8750\n",
      "Elapsed 50.53s, 12.63 s/epoch, 0.00 s/batch, ets 75.80s\n",
      "\n",
      "Test set: Average loss: 0.3471, Accuracy: 8703/10000 (87%)\n",
      "\n",
      "Train Epoch: 4 [1600/60000] Loss: 0.106475 Acc: 1.0000\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.227499 Acc: 0.9375\n",
      "Train Epoch: 4 [4800/60000] Loss: 0.234142 Acc: 0.9375\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.399865 Acc: 0.8750\n",
      "Train Epoch: 4 [8000/60000] Loss: 0.219133 Acc: 0.9375\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.232180 Acc: 0.8750\n",
      "Train Epoch: 4 [11200/60000] Loss: 0.148828 Acc: 0.9375\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.249860 Acc: 0.8750\n",
      "Train Epoch: 4 [14400/60000] Loss: 0.229029 Acc: 0.9375\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.050078 Acc: 1.0000\n",
      "Train Epoch: 4 [17600/60000] Loss: 0.238209 Acc: 0.9375\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.555267 Acc: 0.8125\n",
      "Train Epoch: 4 [20800/60000] Loss: 0.111193 Acc: 1.0000\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.132488 Acc: 1.0000\n",
      "Train Epoch: 4 [24000/60000] Loss: 0.491286 Acc: 0.6250\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.221643 Acc: 0.9375\n",
      "Train Epoch: 4 [27200/60000] Loss: 0.066008 Acc: 1.0000\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.273633 Acc: 0.8125\n",
      "Train Epoch: 4 [30400/60000] Loss: 0.228299 Acc: 0.8750\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.191424 Acc: 0.8750\n",
      "Train Epoch: 4 [33600/60000] Loss: 0.190538 Acc: 0.9375\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.519985 Acc: 0.7500\n",
      "Train Epoch: 4 [36800/60000] Loss: 0.701729 Acc: 0.8125\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.293042 Acc: 0.8750\n",
      "Train Epoch: 4 [40000/60000] Loss: 0.156202 Acc: 0.9375\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.403846 Acc: 0.8125\n",
      "Train Epoch: 4 [43200/60000] Loss: 0.315153 Acc: 0.8750\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.250228 Acc: 0.9375\n",
      "Train Epoch: 4 [46400/60000] Loss: 0.406281 Acc: 0.8125\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.213017 Acc: 1.0000\n",
      "Train Epoch: 4 [49600/60000] Loss: 0.334733 Acc: 0.8750\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.172545 Acc: 1.0000\n",
      "Train Epoch: 4 [52800/60000] Loss: 0.501327 Acc: 0.8750\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.221673 Acc: 0.8750\n",
      "Train Epoch: 4 [56000/60000] Loss: 0.407837 Acc: 0.9375\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.353870 Acc: 0.9375\n",
      "Train Epoch: 4 [59200/60000] Loss: 0.358393 Acc: 0.9375\n",
      "Elapsed 63.65s, 12.73 s/epoch, 0.00 s/batch, ets 63.65s\n",
      "\n",
      "Test set: Average loss: 0.3544, Accuracy: 8691/10000 (87%)\n",
      "\n",
      "Train Epoch: 5 [1600/60000] Loss: 0.236896 Acc: 0.8750\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.514071 Acc: 0.8750\n",
      "Train Epoch: 5 [4800/60000] Loss: 0.261876 Acc: 0.8750\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.056458 Acc: 1.0000\n",
      "Train Epoch: 5 [8000/60000] Loss: 0.335701 Acc: 0.8750\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.405831 Acc: 0.8125\n",
      "Train Epoch: 5 [11200/60000] Loss: 0.109185 Acc: 0.9375\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.348185 Acc: 0.9375\n",
      "Train Epoch: 5 [14400/60000] Loss: 0.383138 Acc: 0.8125\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.314274 Acc: 0.8750\n",
      "Train Epoch: 5 [17600/60000] Loss: 0.427844 Acc: 0.8125\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.366655 Acc: 0.8750\n",
      "Train Epoch: 5 [20800/60000] Loss: 0.332854 Acc: 0.8750\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.331490 Acc: 0.7500\n",
      "Train Epoch: 5 [24000/60000] Loss: 0.523865 Acc: 0.7500\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.257908 Acc: 0.8750\n",
      "Train Epoch: 5 [27200/60000] Loss: 0.137925 Acc: 0.9375\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.449528 Acc: 0.6875\n",
      "Train Epoch: 5 [30400/60000] Loss: 0.289687 Acc: 0.8750\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.382611 Acc: 0.8125\n",
      "Train Epoch: 5 [33600/60000] Loss: 0.113396 Acc: 1.0000\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.108632 Acc: 1.0000\n",
      "Train Epoch: 5 [36800/60000] Loss: 0.254328 Acc: 0.8750\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.459846 Acc: 0.8125\n",
      "Train Epoch: 5 [40000/60000] Loss: 0.271743 Acc: 0.8750\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.111022 Acc: 1.0000\n",
      "Train Epoch: 5 [43200/60000] Loss: 0.247632 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.312355 Acc: 0.8750\n",
      "Train Epoch: 5 [46400/60000] Loss: 1.080577 Acc: 0.5625\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.330099 Acc: 0.8125\n",
      "Train Epoch: 5 [49600/60000] Loss: 0.583056 Acc: 0.7500\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.137902 Acc: 0.9375\n",
      "Train Epoch: 5 [52800/60000] Loss: 0.024771 Acc: 1.0000\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.272580 Acc: 0.9375\n",
      "Train Epoch: 5 [56000/60000] Loss: 0.240721 Acc: 0.8750\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.333531 Acc: 0.9375\n",
      "Train Epoch: 5 [59200/60000] Loss: 0.147960 Acc: 1.0000\n",
      "Elapsed 77.36s, 12.89 s/epoch, 0.00 s/batch, ets 51.58s\n",
      "\n",
      "Test set: Average loss: 0.3400, Accuracy: 8749/10000 (87%)\n",
      "\n",
      "Train Epoch: 6 [1600/60000] Loss: 0.182054 Acc: 1.0000\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.120436 Acc: 0.9375\n",
      "Train Epoch: 6 [4800/60000] Loss: 0.432359 Acc: 0.8125\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.386475 Acc: 0.8125\n",
      "Train Epoch: 6 [8000/60000] Loss: 0.161817 Acc: 0.9375\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.164840 Acc: 0.9375\n",
      "Train Epoch: 6 [11200/60000] Loss: 0.557993 Acc: 0.8125\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.178153 Acc: 0.9375\n",
      "Train Epoch: 6 [14400/60000] Loss: 0.117421 Acc: 1.0000\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.105181 Acc: 1.0000\n",
      "Train Epoch: 6 [17600/60000] Loss: 0.074607 Acc: 1.0000\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.076620 Acc: 1.0000\n",
      "Train Epoch: 6 [20800/60000] Loss: 0.295786 Acc: 0.9375\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.532496 Acc: 0.8750\n",
      "Train Epoch: 6 [24000/60000] Loss: 0.179316 Acc: 0.9375\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.263953 Acc: 0.8125\n",
      "Train Epoch: 6 [27200/60000] Loss: 0.281744 Acc: 0.8750\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.423964 Acc: 0.8125\n",
      "Train Epoch: 6 [30400/60000] Loss: 0.180801 Acc: 1.0000\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.387791 Acc: 0.8125\n",
      "Train Epoch: 6 [33600/60000] Loss: 0.219860 Acc: 0.8750\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.205793 Acc: 0.9375\n",
      "Train Epoch: 6 [36800/60000] Loss: 0.061162 Acc: 1.0000\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.418060 Acc: 0.8750\n",
      "Train Epoch: 6 [40000/60000] Loss: 0.057683 Acc: 1.0000\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.156239 Acc: 1.0000\n",
      "Train Epoch: 6 [43200/60000] Loss: 0.525608 Acc: 0.6875\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.479421 Acc: 0.8750\n",
      "Train Epoch: 6 [46400/60000] Loss: 0.100566 Acc: 1.0000\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.302995 Acc: 0.8125\n",
      "Train Epoch: 6 [49600/60000] Loss: 0.475636 Acc: 0.6875\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.247942 Acc: 0.8125\n",
      "Train Epoch: 6 [52800/60000] Loss: 0.291707 Acc: 0.8750\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.157363 Acc: 0.8750\n",
      "Train Epoch: 6 [56000/60000] Loss: 0.262128 Acc: 0.8750\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.260088 Acc: 0.8125\n",
      "Train Epoch: 6 [59200/60000] Loss: 0.143180 Acc: 0.9375\n",
      "Elapsed 91.05s, 13.01 s/epoch, 0.00 s/batch, ets 39.02s\n",
      "\n",
      "Test set: Average loss: 0.3184, Accuracy: 8840/10000 (88%)\n",
      "\n",
      "Train Epoch: 7 [1600/60000] Loss: 0.327031 Acc: 0.8750\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.398978 Acc: 0.8750\n",
      "Train Epoch: 7 [4800/60000] Loss: 0.228811 Acc: 0.9375\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.976013 Acc: 0.6875\n",
      "Train Epoch: 7 [8000/60000] Loss: 0.278212 Acc: 0.9375\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.215687 Acc: 0.8750\n",
      "Train Epoch: 7 [11200/60000] Loss: 0.189421 Acc: 0.9375\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.278588 Acc: 0.8750\n",
      "Train Epoch: 7 [14400/60000] Loss: 0.442112 Acc: 0.7500\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.244258 Acc: 0.9375\n",
      "Train Epoch: 7 [17600/60000] Loss: 0.295150 Acc: 0.8750\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.619725 Acc: 0.8125\n",
      "Train Epoch: 7 [20800/60000] Loss: 0.062573 Acc: 1.0000\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.069969 Acc: 1.0000\n",
      "Train Epoch: 7 [24000/60000] Loss: 0.041783 Acc: 1.0000\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.382474 Acc: 0.8750\n",
      "Train Epoch: 7 [27200/60000] Loss: 0.205156 Acc: 0.9375\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.237407 Acc: 0.8125\n",
      "Train Epoch: 7 [30400/60000] Loss: 0.280311 Acc: 0.8750\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.107740 Acc: 0.9375\n",
      "Train Epoch: 7 [33600/60000] Loss: 0.183186 Acc: 0.9375\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.442904 Acc: 0.8750\n",
      "Train Epoch: 7 [36800/60000] Loss: 0.385060 Acc: 0.8750\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.867784 Acc: 0.6250\n",
      "Train Epoch: 7 [40000/60000] Loss: 0.031347 Acc: 1.0000\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.176779 Acc: 0.9375\n",
      "Train Epoch: 7 [43200/60000] Loss: 0.053716 Acc: 1.0000\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.190876 Acc: 0.8750\n",
      "Train Epoch: 7 [46400/60000] Loss: 0.143237 Acc: 0.9375\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.073295 Acc: 1.0000\n",
      "Train Epoch: 7 [49600/60000] Loss: 0.174039 Acc: 0.9375\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.220051 Acc: 0.9375\n",
      "Train Epoch: 7 [52800/60000] Loss: 0.243613 Acc: 0.9375\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.188475 Acc: 0.8750\n",
      "Train Epoch: 7 [56000/60000] Loss: 0.283730 Acc: 0.8750\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.270691 Acc: 0.8125\n",
      "Train Epoch: 7 [59200/60000] Loss: 0.247357 Acc: 0.8750\n",
      "Elapsed 104.99s, 13.12 s/epoch, 0.00 s/batch, ets 26.25s\n",
      "\n",
      "Test set: Average loss: 0.3098, Accuracy: 8841/10000 (88%)\n",
      "\n",
      "Train Epoch: 8 [1600/60000] Loss: 0.035532 Acc: 1.0000\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.553940 Acc: 0.8125\n",
      "Train Epoch: 8 [4800/60000] Loss: 0.327379 Acc: 0.8125\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.356662 Acc: 0.8125\n",
      "Train Epoch: 8 [8000/60000] Loss: 0.134350 Acc: 0.9375\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.250176 Acc: 0.9375\n",
      "Train Epoch: 8 [11200/60000] Loss: 0.201128 Acc: 0.9375\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.194096 Acc: 0.9375\n",
      "Train Epoch: 8 [14400/60000] Loss: 0.892821 Acc: 0.8125\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.064297 Acc: 1.0000\n",
      "Train Epoch: 8 [17600/60000] Loss: 0.352897 Acc: 0.8750\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.052127 Acc: 1.0000\n",
      "Train Epoch: 8 [20800/60000] Loss: 0.318237 Acc: 0.8125\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.180639 Acc: 0.9375\n",
      "Train Epoch: 8 [24000/60000] Loss: 0.392518 Acc: 0.7500\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.183007 Acc: 0.9375\n",
      "Train Epoch: 8 [27200/60000] Loss: 0.116467 Acc: 1.0000\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.095421 Acc: 1.0000\n",
      "Train Epoch: 8 [30400/60000] Loss: 0.396828 Acc: 0.9375\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.080345 Acc: 1.0000\n",
      "Train Epoch: 8 [33600/60000] Loss: 0.248276 Acc: 0.8125\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.058705 Acc: 1.0000\n",
      "Train Epoch: 8 [36800/60000] Loss: 0.229723 Acc: 0.8750\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.346227 Acc: 0.8750\n",
      "Train Epoch: 8 [40000/60000] Loss: 0.222125 Acc: 0.9375\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.118216 Acc: 0.9375\n",
      "Train Epoch: 8 [43200/60000] Loss: 0.324918 Acc: 0.8750\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.228469 Acc: 0.8125\n",
      "Train Epoch: 8 [46400/60000] Loss: 0.382558 Acc: 0.8125\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.221502 Acc: 0.8750\n",
      "Train Epoch: 8 [49600/60000] Loss: 0.100501 Acc: 1.0000\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.070001 Acc: 1.0000\n",
      "Train Epoch: 8 [52800/60000] Loss: 0.341212 Acc: 0.8750\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.030752 Acc: 1.0000\n",
      "Train Epoch: 8 [56000/60000] Loss: 0.368449 Acc: 0.8750\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.112749 Acc: 1.0000\n",
      "Train Epoch: 8 [59200/60000] Loss: 0.297962 Acc: 0.8750\n",
      "Elapsed 118.22s, 13.14 s/epoch, 0.00 s/batch, ets 13.14s\n",
      "\n",
      "Test set: Average loss: 0.2934, Accuracy: 8922/10000 (89%)\n",
      "\n",
      "Train Epoch: 9 [1600/60000] Loss: 0.238987 Acc: 0.9375\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.123639 Acc: 0.9375\n",
      "Train Epoch: 9 [4800/60000] Loss: 0.045065 Acc: 1.0000\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.120643 Acc: 1.0000\n",
      "Train Epoch: 9 [8000/60000] Loss: 0.008599 Acc: 1.0000\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.032112 Acc: 1.0000\n",
      "Train Epoch: 9 [11200/60000] Loss: 0.131575 Acc: 0.8750\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.152390 Acc: 0.9375\n",
      "Train Epoch: 9 [14400/60000] Loss: 0.139040 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.223264 Acc: 0.9375\n",
      "Train Epoch: 9 [17600/60000] Loss: 0.188701 Acc: 0.9375\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.198801 Acc: 0.9375\n",
      "Train Epoch: 9 [20800/60000] Loss: 0.276463 Acc: 0.9375\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.104825 Acc: 1.0000\n",
      "Train Epoch: 9 [24000/60000] Loss: 0.189298 Acc: 0.9375\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.199795 Acc: 0.9375\n",
      "Train Epoch: 9 [27200/60000] Loss: 0.220818 Acc: 0.8750\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.433181 Acc: 0.7500\n",
      "Train Epoch: 9 [30400/60000] Loss: 0.176915 Acc: 0.9375\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.372961 Acc: 0.8125\n",
      "Train Epoch: 9 [33600/60000] Loss: 0.928047 Acc: 0.6250\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.304954 Acc: 0.8750\n",
      "Train Epoch: 9 [36800/60000] Loss: 0.308768 Acc: 0.8750\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.460805 Acc: 0.8750\n",
      "Train Epoch: 9 [40000/60000] Loss: 0.046654 Acc: 1.0000\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.132614 Acc: 0.9375\n",
      "Train Epoch: 9 [43200/60000] Loss: 0.162007 Acc: 1.0000\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.672525 Acc: 0.7500\n",
      "Train Epoch: 9 [46400/60000] Loss: 0.153228 Acc: 0.9375\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.210178 Acc: 0.9375\n",
      "Train Epoch: 9 [49600/60000] Loss: 0.159034 Acc: 0.8750\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.697054 Acc: 0.8750\n",
      "Train Epoch: 9 [52800/60000] Loss: 0.203079 Acc: 0.8750\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.025346 Acc: 1.0000\n",
      "Train Epoch: 9 [56000/60000] Loss: 0.182162 Acc: 0.9375\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.083205 Acc: 1.0000\n",
      "Train Epoch: 9 [59200/60000] Loss: 0.386627 Acc: 0.8125\n",
      "Elapsed 132.37s, 13.24 s/epoch, 0.00 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2964, Accuracy: 8908/10000 (89%)\n",
      "\n",
      "Total time: 133.90, Best Loss: 0.293\n",
      "Train Epoch: 0 [1600/60000] Loss: 1.936184 Acc: 0.5000\n",
      "Train Epoch: 0 [3200/60000] Loss: 1.133034 Acc: 0.8125\n",
      "Train Epoch: 0 [4800/60000] Loss: 0.843644 Acc: 0.8750\n",
      "Train Epoch: 0 [6400/60000] Loss: 0.919838 Acc: 0.6875\n",
      "Train Epoch: 0 [8000/60000] Loss: 0.478753 Acc: 0.8125\n",
      "Train Epoch: 0 [9600/60000] Loss: 0.503087 Acc: 0.8125\n",
      "Train Epoch: 0 [11200/60000] Loss: 0.634113 Acc: 0.6875\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.849878 Acc: 0.6250\n",
      "Train Epoch: 0 [14400/60000] Loss: 0.559013 Acc: 0.7500\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.455215 Acc: 0.8750\n",
      "Train Epoch: 0 [17600/60000] Loss: 0.336670 Acc: 0.9375\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.468018 Acc: 0.8125\n",
      "Train Epoch: 0 [20800/60000] Loss: 0.604588 Acc: 0.7500\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.555955 Acc: 0.8125\n",
      "Train Epoch: 0 [24000/60000] Loss: 0.177352 Acc: 0.9375\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.602609 Acc: 0.7500\n",
      "Train Epoch: 0 [27200/60000] Loss: 0.505278 Acc: 0.6875\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.153763 Acc: 1.0000\n",
      "Train Epoch: 0 [30400/60000] Loss: 0.526666 Acc: 0.8125\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.642271 Acc: 0.7500\n",
      "Train Epoch: 0 [33600/60000] Loss: 0.173463 Acc: 1.0000\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.700652 Acc: 0.6875\n",
      "Train Epoch: 0 [36800/60000] Loss: 0.651681 Acc: 0.6875\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.126429 Acc: 0.9375\n",
      "Train Epoch: 0 [40000/60000] Loss: 0.366692 Acc: 0.8125\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.270012 Acc: 0.9375\n",
      "Train Epoch: 0 [43200/60000] Loss: 0.412475 Acc: 0.8750\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.412352 Acc: 0.8750\n",
      "Train Epoch: 0 [46400/60000] Loss: 0.210947 Acc: 1.0000\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.800137 Acc: 0.6875\n",
      "Train Epoch: 0 [49600/60000] Loss: 0.537225 Acc: 0.8125\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.192447 Acc: 0.9375\n",
      "Train Epoch: 0 [52800/60000] Loss: 0.241530 Acc: 0.9375\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.523751 Acc: 0.8125\n",
      "Train Epoch: 0 [56000/60000] Loss: 0.273529 Acc: 0.8750\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.117426 Acc: 0.9375\n",
      "Train Epoch: 0 [59200/60000] Loss: 0.193798 Acc: 1.0000\n",
      "Elapsed 12.92s, 12.92 s/epoch, 0.00 s/batch, ets 116.25s\n",
      "\n",
      "Test set: Average loss: 0.4222, Accuracy: 8462/10000 (85%)\n",
      "\n",
      "Train Epoch: 1 [1600/60000] Loss: 0.288089 Acc: 0.9375\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.435602 Acc: 0.8125\n",
      "Train Epoch: 1 [4800/60000] Loss: 0.276335 Acc: 0.9375\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.277686 Acc: 0.9375\n",
      "Train Epoch: 1 [8000/60000] Loss: 0.638219 Acc: 0.8125\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.248820 Acc: 0.8750\n",
      "Train Epoch: 1 [11200/60000] Loss: 0.317127 Acc: 0.8750\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.556408 Acc: 0.7500\n",
      "Train Epoch: 1 [14400/60000] Loss: 0.392590 Acc: 0.8125\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.246558 Acc: 0.9375\n",
      "Train Epoch: 1 [17600/60000] Loss: 0.420043 Acc: 0.8125\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.777843 Acc: 0.7500\n",
      "Train Epoch: 1 [20800/60000] Loss: 0.439510 Acc: 0.7500\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.349476 Acc: 0.7500\n",
      "Train Epoch: 1 [24000/60000] Loss: 0.215251 Acc: 0.9375\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.459224 Acc: 0.8125\n",
      "Train Epoch: 1 [27200/60000] Loss: 0.130359 Acc: 1.0000\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.349346 Acc: 0.8750\n",
      "Train Epoch: 1 [30400/60000] Loss: 0.357585 Acc: 0.8125\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.288141 Acc: 0.8750\n",
      "Train Epoch: 1 [33600/60000] Loss: 0.256787 Acc: 0.8750\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.173699 Acc: 1.0000\n",
      "Train Epoch: 1 [36800/60000] Loss: 0.291541 Acc: 0.9375\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.349335 Acc: 0.8125\n",
      "Train Epoch: 1 [40000/60000] Loss: 0.117790 Acc: 1.0000\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.392763 Acc: 0.9375\n",
      "Train Epoch: 1 [43200/60000] Loss: 0.344728 Acc: 0.9375\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.272756 Acc: 0.9375\n",
      "Train Epoch: 1 [46400/60000] Loss: 0.505704 Acc: 0.8125\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.331509 Acc: 0.8125\n",
      "Train Epoch: 1 [49600/60000] Loss: 0.361687 Acc: 0.8750\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.126844 Acc: 0.9375\n",
      "Train Epoch: 1 [52800/60000] Loss: 0.244104 Acc: 0.8750\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.662852 Acc: 0.7500\n",
      "Train Epoch: 1 [56000/60000] Loss: 0.235032 Acc: 0.9375\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.301440 Acc: 0.8750\n",
      "Train Epoch: 1 [59200/60000] Loss: 0.221645 Acc: 0.9375\n",
      "Elapsed 27.78s, 13.89 s/epoch, 0.00 s/batch, ets 111.13s\n",
      "\n",
      "Test set: Average loss: 0.3523, Accuracy: 8682/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [1600/60000] Loss: 0.311972 Acc: 0.8750\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.700715 Acc: 0.8125\n",
      "Train Epoch: 2 [4800/60000] Loss: 0.491267 Acc: 0.8125\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.249366 Acc: 0.8750\n",
      "Train Epoch: 2 [8000/60000] Loss: 0.179785 Acc: 0.9375\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.062328 Acc: 1.0000\n",
      "Train Epoch: 2 [11200/60000] Loss: 0.208734 Acc: 0.9375\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.268747 Acc: 0.9375\n",
      "Train Epoch: 2 [14400/60000] Loss: 0.636196 Acc: 0.6875\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.230591 Acc: 0.8750\n",
      "Train Epoch: 2 [17600/60000] Loss: 0.543165 Acc: 0.7500\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.782119 Acc: 0.7500\n",
      "Train Epoch: 2 [20800/60000] Loss: 0.415413 Acc: 0.8125\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.318407 Acc: 0.8125\n",
      "Train Epoch: 2 [24000/60000] Loss: 0.249642 Acc: 0.9375\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.193996 Acc: 0.9375\n",
      "Train Epoch: 2 [27200/60000] Loss: 0.212666 Acc: 0.8750\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.215276 Acc: 0.9375\n",
      "Train Epoch: 2 [30400/60000] Loss: 0.412712 Acc: 0.8750\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.115222 Acc: 1.0000\n",
      "Train Epoch: 2 [33600/60000] Loss: 0.503286 Acc: 0.8125\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.322705 Acc: 0.8125\n",
      "Train Epoch: 2 [36800/60000] Loss: 0.416553 Acc: 0.8125\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.240762 Acc: 0.8750\n",
      "Train Epoch: 2 [40000/60000] Loss: 0.653567 Acc: 0.8125\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.612526 Acc: 0.8125\n",
      "Train Epoch: 2 [43200/60000] Loss: 0.236272 Acc: 0.9375\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.274814 Acc: 0.9375\n",
      "Train Epoch: 2 [46400/60000] Loss: 0.596036 Acc: 0.8125\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.158234 Acc: 0.9375\n",
      "Train Epoch: 2 [49600/60000] Loss: 0.658710 Acc: 0.6875\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.207186 Acc: 0.9375\n",
      "Train Epoch: 2 [52800/60000] Loss: 0.119507 Acc: 1.0000\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.278072 Acc: 0.9375\n",
      "Train Epoch: 2 [56000/60000] Loss: 0.318953 Acc: 0.8750\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.268582 Acc: 0.9375\n",
      "Train Epoch: 2 [59200/60000] Loss: 0.287690 Acc: 0.8125\n",
      "Elapsed 42.42s, 14.14 s/epoch, 0.00 s/batch, ets 98.98s\n",
      "\n",
      "Test set: Average loss: 0.3445, Accuracy: 8733/10000 (87%)\n",
      "\n",
      "Train Epoch: 3 [1600/60000] Loss: 0.386812 Acc: 0.8750\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.071845 Acc: 1.0000\n",
      "Train Epoch: 3 [4800/60000] Loss: 0.168389 Acc: 0.8750\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.309254 Acc: 0.8125\n",
      "Train Epoch: 3 [8000/60000] Loss: 0.307050 Acc: 0.8750\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.643112 Acc: 0.8125\n",
      "Train Epoch: 3 [11200/60000] Loss: 0.149991 Acc: 0.9375\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.415426 Acc: 0.8125\n",
      "Train Epoch: 3 [14400/60000] Loss: 0.216297 Acc: 0.9375\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.055931 Acc: 1.0000\n",
      "Train Epoch: 3 [17600/60000] Loss: 0.834156 Acc: 0.6875\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.187054 Acc: 1.0000\n",
      "Train Epoch: 3 [20800/60000] Loss: 0.062473 Acc: 1.0000\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.582486 Acc: 0.9375\n",
      "Train Epoch: 3 [24000/60000] Loss: 0.263800 Acc: 0.9375\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.336310 Acc: 0.8750\n",
      "Train Epoch: 3 [27200/60000] Loss: 0.216458 Acc: 0.9375\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.186234 Acc: 0.9375\n",
      "Train Epoch: 3 [30400/60000] Loss: 0.314690 Acc: 0.8750\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.266950 Acc: 0.9375\n",
      "Train Epoch: 3 [33600/60000] Loss: 0.372118 Acc: 0.8750\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.156364 Acc: 0.9375\n",
      "Train Epoch: 3 [36800/60000] Loss: 0.330703 Acc: 0.8125\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.170517 Acc: 0.9375\n",
      "Train Epoch: 3 [40000/60000] Loss: 1.040742 Acc: 0.6875\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.394282 Acc: 0.8125\n",
      "Train Epoch: 3 [43200/60000] Loss: 0.426685 Acc: 0.7500\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.092649 Acc: 0.9375\n",
      "Train Epoch: 3 [46400/60000] Loss: 0.210684 Acc: 0.9375\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.927355 Acc: 0.6875\n",
      "Train Epoch: 3 [49600/60000] Loss: 0.235071 Acc: 0.8750\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.880303 Acc: 0.7500\n",
      "Train Epoch: 3 [52800/60000] Loss: 0.295034 Acc: 0.8750\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.380662 Acc: 0.8750\n",
      "Train Epoch: 3 [56000/60000] Loss: 0.238899 Acc: 0.9375\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.193332 Acc: 0.9375\n",
      "Train Epoch: 3 [59200/60000] Loss: 0.465521 Acc: 0.8750\n",
      "Elapsed 57.33s, 14.33 s/epoch, 0.00 s/batch, ets 86.00s\n",
      "\n",
      "Test set: Average loss: 0.3236, Accuracy: 8791/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [1600/60000] Loss: 0.118151 Acc: 1.0000\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.245559 Acc: 0.9375\n",
      "Train Epoch: 4 [4800/60000] Loss: 0.120436 Acc: 0.9375\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.286599 Acc: 0.9375\n",
      "Train Epoch: 4 [8000/60000] Loss: 0.216286 Acc: 0.9375\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.207125 Acc: 0.9375\n",
      "Train Epoch: 4 [11200/60000] Loss: 0.174170 Acc: 0.9375\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.326866 Acc: 0.8750\n",
      "Train Epoch: 4 [14400/60000] Loss: 0.228698 Acc: 0.9375\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.044347 Acc: 1.0000\n",
      "Train Epoch: 4 [17600/60000] Loss: 0.221414 Acc: 0.8750\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.299845 Acc: 0.8125\n",
      "Train Epoch: 4 [20800/60000] Loss: 0.087747 Acc: 1.0000\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.094265 Acc: 1.0000\n",
      "Train Epoch: 4 [24000/60000] Loss: 0.368362 Acc: 0.8750\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.223478 Acc: 0.9375\n",
      "Train Epoch: 4 [27200/60000] Loss: 0.042693 Acc: 1.0000\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.286293 Acc: 0.8750\n",
      "Train Epoch: 4 [30400/60000] Loss: 0.087134 Acc: 1.0000\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.162829 Acc: 0.8750\n",
      "Train Epoch: 4 [33600/60000] Loss: 0.122460 Acc: 1.0000\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.428930 Acc: 0.8125\n",
      "Train Epoch: 4 [36800/60000] Loss: 0.762933 Acc: 0.8125\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.348058 Acc: 0.8125\n",
      "Train Epoch: 4 [40000/60000] Loss: 0.169000 Acc: 0.8750\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.471554 Acc: 0.8125\n",
      "Train Epoch: 4 [43200/60000] Loss: 0.254073 Acc: 0.8750\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.155173 Acc: 0.9375\n",
      "Train Epoch: 4 [46400/60000] Loss: 0.335456 Acc: 0.8750\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.151430 Acc: 1.0000\n",
      "Train Epoch: 4 [49600/60000] Loss: 0.447488 Acc: 0.8750\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.120638 Acc: 1.0000\n",
      "Train Epoch: 4 [52800/60000] Loss: 0.638520 Acc: 0.8125\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.249873 Acc: 0.8750\n",
      "Train Epoch: 4 [56000/60000] Loss: 0.266811 Acc: 0.8750\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.379175 Acc: 0.8750\n",
      "Train Epoch: 4 [59200/60000] Loss: 0.350943 Acc: 0.8750\n",
      "Elapsed 72.02s, 14.40 s/epoch, 0.00 s/batch, ets 72.02s\n",
      "\n",
      "Test set: Average loss: 0.3083, Accuracy: 8886/10000 (89%)\n",
      "\n",
      "Train Epoch: 5 [1600/60000] Loss: 0.333548 Acc: 0.8750\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.177883 Acc: 0.8750\n",
      "Train Epoch: 5 [4800/60000] Loss: 0.120231 Acc: 1.0000\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.085935 Acc: 1.0000\n",
      "Train Epoch: 5 [8000/60000] Loss: 0.404583 Acc: 0.8750\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.299195 Acc: 0.8750\n",
      "Train Epoch: 5 [11200/60000] Loss: 0.182839 Acc: 0.8750\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.177468 Acc: 0.9375\n",
      "Train Epoch: 5 [14400/60000] Loss: 0.243128 Acc: 0.8125\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.251471 Acc: 0.8750\n",
      "Train Epoch: 5 [17600/60000] Loss: 0.344080 Acc: 0.8125\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.358970 Acc: 0.8750\n",
      "Train Epoch: 5 [20800/60000] Loss: 0.212791 Acc: 0.9375\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.322861 Acc: 0.8125\n",
      "Train Epoch: 5 [24000/60000] Loss: 0.308435 Acc: 0.8750\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.145161 Acc: 0.9375\n",
      "Train Epoch: 5 [27200/60000] Loss: 0.131944 Acc: 0.9375\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.346943 Acc: 0.8125\n",
      "Train Epoch: 5 [30400/60000] Loss: 0.363029 Acc: 0.9375\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.263108 Acc: 0.8750\n",
      "Train Epoch: 5 [33600/60000] Loss: 0.101997 Acc: 1.0000\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.100868 Acc: 0.9375\n",
      "Train Epoch: 5 [36800/60000] Loss: 0.255963 Acc: 0.8750\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.249699 Acc: 0.8750\n",
      "Train Epoch: 5 [40000/60000] Loss: 0.155497 Acc: 0.9375\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.116502 Acc: 0.9375\n",
      "Train Epoch: 5 [43200/60000] Loss: 0.228824 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.329521 Acc: 0.8125\n",
      "Train Epoch: 5 [46400/60000] Loss: 0.778965 Acc: 0.6875\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.419545 Acc: 0.8125\n",
      "Train Epoch: 5 [49600/60000] Loss: 0.562874 Acc: 0.7500\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.147492 Acc: 0.9375\n",
      "Train Epoch: 5 [52800/60000] Loss: 0.032640 Acc: 1.0000\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.277730 Acc: 0.8125\n",
      "Train Epoch: 5 [56000/60000] Loss: 0.219817 Acc: 0.8750\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.250361 Acc: 0.9375\n",
      "Train Epoch: 5 [59200/60000] Loss: 0.152070 Acc: 0.9375\n",
      "Elapsed 86.63s, 14.44 s/epoch, 0.00 s/batch, ets 57.75s\n",
      "\n",
      "Test set: Average loss: 0.3057, Accuracy: 8902/10000 (89%)\n",
      "\n",
      "Train Epoch: 6 [1600/60000] Loss: 0.160494 Acc: 0.9375\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.049866 Acc: 1.0000\n",
      "Train Epoch: 6 [4800/60000] Loss: 0.285785 Acc: 0.8125\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.436015 Acc: 0.8125\n",
      "Train Epoch: 6 [8000/60000] Loss: 0.113458 Acc: 1.0000\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.118960 Acc: 0.9375\n",
      "Train Epoch: 6 [11200/60000] Loss: 0.680922 Acc: 0.7500\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.244038 Acc: 0.9375\n",
      "Train Epoch: 6 [14400/60000] Loss: 0.112511 Acc: 1.0000\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.243234 Acc: 0.9375\n",
      "Train Epoch: 6 [17600/60000] Loss: 0.082101 Acc: 0.9375\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.055943 Acc: 1.0000\n",
      "Train Epoch: 6 [20800/60000] Loss: 0.485610 Acc: 0.7500\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.441540 Acc: 0.8750\n",
      "Train Epoch: 6 [24000/60000] Loss: 0.214950 Acc: 0.8125\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.188468 Acc: 0.9375\n",
      "Train Epoch: 6 [27200/60000] Loss: 0.114052 Acc: 1.0000\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.511924 Acc: 0.8750\n",
      "Train Epoch: 6 [30400/60000] Loss: 0.180379 Acc: 0.9375\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.196578 Acc: 0.8750\n",
      "Train Epoch: 6 [33600/60000] Loss: 0.320036 Acc: 0.9375\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.201171 Acc: 0.8750\n",
      "Train Epoch: 6 [36800/60000] Loss: 0.108724 Acc: 0.9375\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.405744 Acc: 0.8750\n",
      "Train Epoch: 6 [40000/60000] Loss: 0.069557 Acc: 1.0000\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.171398 Acc: 1.0000\n",
      "Train Epoch: 6 [43200/60000] Loss: 0.488117 Acc: 0.8125\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.448480 Acc: 0.8750\n",
      "Train Epoch: 6 [46400/60000] Loss: 0.096458 Acc: 1.0000\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.112759 Acc: 1.0000\n",
      "Train Epoch: 6 [49600/60000] Loss: 0.619122 Acc: 0.8125\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.232389 Acc: 0.8125\n",
      "Train Epoch: 6 [52800/60000] Loss: 0.545866 Acc: 0.8125\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.179637 Acc: 0.9375\n",
      "Train Epoch: 6 [56000/60000] Loss: 0.184129 Acc: 0.9375\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.238636 Acc: 0.8750\n",
      "Train Epoch: 6 [59200/60000] Loss: 0.051769 Acc: 1.0000\n",
      "Elapsed 101.53s, 14.50 s/epoch, 0.00 s/batch, ets 43.51s\n",
      "\n",
      "Test set: Average loss: 0.2917, Accuracy: 8958/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [1600/60000] Loss: 0.461375 Acc: 0.8750\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.251420 Acc: 0.8125\n",
      "Train Epoch: 7 [4800/60000] Loss: 0.119880 Acc: 0.9375\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.928036 Acc: 0.6875\n",
      "Train Epoch: 7 [8000/60000] Loss: 0.289421 Acc: 0.9375\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.234557 Acc: 0.8750\n",
      "Train Epoch: 7 [11200/60000] Loss: 0.243792 Acc: 0.9375\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.340718 Acc: 0.8125\n",
      "Train Epoch: 7 [14400/60000] Loss: 0.332806 Acc: 0.8750\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.326141 Acc: 0.8750\n",
      "Train Epoch: 7 [17600/60000] Loss: 0.379102 Acc: 0.7500\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.499869 Acc: 0.8750\n",
      "Train Epoch: 7 [20800/60000] Loss: 0.036011 Acc: 1.0000\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.049505 Acc: 1.0000\n",
      "Train Epoch: 7 [24000/60000] Loss: 0.032117 Acc: 1.0000\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.362747 Acc: 0.8750\n",
      "Train Epoch: 7 [27200/60000] Loss: 0.072730 Acc: 0.9375\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.307791 Acc: 0.9375\n",
      "Train Epoch: 7 [30400/60000] Loss: 0.185024 Acc: 0.9375\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.096858 Acc: 0.9375\n",
      "Train Epoch: 7 [33600/60000] Loss: 0.157409 Acc: 0.9375\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.373510 Acc: 0.8750\n",
      "Train Epoch: 7 [36800/60000] Loss: 0.293288 Acc: 0.8750\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.715186 Acc: 0.6250\n",
      "Train Epoch: 7 [40000/60000] Loss: 0.027337 Acc: 1.0000\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.205906 Acc: 0.9375\n",
      "Train Epoch: 7 [43200/60000] Loss: 0.054955 Acc: 1.0000\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.159196 Acc: 0.8750\n",
      "Train Epoch: 7 [46400/60000] Loss: 0.158311 Acc: 0.9375\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.057028 Acc: 1.0000\n",
      "Train Epoch: 7 [49600/60000] Loss: 0.113513 Acc: 1.0000\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.191215 Acc: 0.9375\n",
      "Train Epoch: 7 [52800/60000] Loss: 0.201754 Acc: 0.9375\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.221864 Acc: 0.8125\n",
      "Train Epoch: 7 [56000/60000] Loss: 0.332138 Acc: 0.8750\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.244389 Acc: 0.8750\n",
      "Train Epoch: 7 [59200/60000] Loss: 0.237797 Acc: 0.8750\n",
      "Elapsed 115.75s, 14.47 s/epoch, 0.00 s/batch, ets 28.94s\n",
      "\n",
      "Test set: Average loss: 0.2851, Accuracy: 8955/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [1600/60000] Loss: 0.022528 Acc: 1.0000\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.622001 Acc: 0.7500\n",
      "Train Epoch: 8 [4800/60000] Loss: 0.395030 Acc: 0.8125\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.464154 Acc: 0.7500\n",
      "Train Epoch: 8 [8000/60000] Loss: 0.040543 Acc: 1.0000\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.277184 Acc: 0.9375\n",
      "Train Epoch: 8 [11200/60000] Loss: 0.277799 Acc: 0.9375\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.326051 Acc: 0.8750\n",
      "Train Epoch: 8 [14400/60000] Loss: 0.663304 Acc: 0.8750\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.126412 Acc: 1.0000\n",
      "Train Epoch: 8 [17600/60000] Loss: 0.410161 Acc: 0.8750\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.039159 Acc: 1.0000\n",
      "Train Epoch: 8 [20800/60000] Loss: 0.269879 Acc: 0.8750\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.236186 Acc: 0.8750\n",
      "Train Epoch: 8 [24000/60000] Loss: 0.257090 Acc: 0.8750\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.205964 Acc: 0.9375\n",
      "Train Epoch: 8 [27200/60000] Loss: 0.077097 Acc: 1.0000\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.035106 Acc: 1.0000\n",
      "Train Epoch: 8 [30400/60000] Loss: 0.222491 Acc: 0.9375\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.104473 Acc: 0.9375\n",
      "Train Epoch: 8 [33600/60000] Loss: 0.157844 Acc: 0.9375\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.037602 Acc: 1.0000\n",
      "Train Epoch: 8 [36800/60000] Loss: 0.285941 Acc: 0.8750\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.393077 Acc: 0.8750\n",
      "Train Epoch: 8 [40000/60000] Loss: 0.147362 Acc: 0.9375\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.121982 Acc: 1.0000\n",
      "Train Epoch: 8 [43200/60000] Loss: 0.190180 Acc: 0.9375\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.361843 Acc: 0.8125\n",
      "Train Epoch: 8 [46400/60000] Loss: 0.636430 Acc: 0.8125\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.137463 Acc: 0.8750\n",
      "Train Epoch: 8 [49600/60000] Loss: 0.116485 Acc: 0.9375\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.063736 Acc: 1.0000\n",
      "Train Epoch: 8 [52800/60000] Loss: 0.219769 Acc: 0.8750\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.018556 Acc: 1.0000\n",
      "Train Epoch: 8 [56000/60000] Loss: 0.782055 Acc: 0.8125\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.224349 Acc: 0.9375\n",
      "Train Epoch: 8 [59200/60000] Loss: 0.325630 Acc: 0.8125\n",
      "Elapsed 130.67s, 14.52 s/epoch, 0.00 s/batch, ets 14.52s\n",
      "\n",
      "Test set: Average loss: 0.2805, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [1600/60000] Loss: 0.191096 Acc: 0.9375\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.296327 Acc: 0.9375\n",
      "Train Epoch: 9 [4800/60000] Loss: 0.046470 Acc: 1.0000\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.058778 Acc: 1.0000\n",
      "Train Epoch: 9 [8000/60000] Loss: 0.046866 Acc: 1.0000\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.030286 Acc: 1.0000\n",
      "Train Epoch: 9 [11200/60000] Loss: 0.121345 Acc: 0.9375\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.146054 Acc: 0.9375\n",
      "Train Epoch: 9 [14400/60000] Loss: 0.173945 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.082372 Acc: 1.0000\n",
      "Train Epoch: 9 [17600/60000] Loss: 0.185836 Acc: 0.9375\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.302732 Acc: 0.9375\n",
      "Train Epoch: 9 [20800/60000] Loss: 0.151201 Acc: 0.9375\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.111829 Acc: 0.9375\n",
      "Train Epoch: 9 [24000/60000] Loss: 0.248640 Acc: 0.8750\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.116595 Acc: 1.0000\n",
      "Train Epoch: 9 [27200/60000] Loss: 0.191066 Acc: 0.9375\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.432855 Acc: 0.9375\n",
      "Train Epoch: 9 [30400/60000] Loss: 0.090892 Acc: 1.0000\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.319430 Acc: 0.8125\n",
      "Train Epoch: 9 [33600/60000] Loss: 0.725746 Acc: 0.7500\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.365531 Acc: 0.8125\n",
      "Train Epoch: 9 [36800/60000] Loss: 0.268744 Acc: 0.9375\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.176077 Acc: 1.0000\n",
      "Train Epoch: 9 [40000/60000] Loss: 0.033904 Acc: 1.0000\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.190884 Acc: 0.9375\n",
      "Train Epoch: 9 [43200/60000] Loss: 0.185487 Acc: 0.8750\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.431242 Acc: 0.7500\n",
      "Train Epoch: 9 [46400/60000] Loss: 0.153368 Acc: 0.8750\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.215929 Acc: 0.9375\n",
      "Train Epoch: 9 [49600/60000] Loss: 0.127416 Acc: 0.9375\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.723174 Acc: 0.8125\n",
      "Train Epoch: 9 [52800/60000] Loss: 0.075843 Acc: 1.0000\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.019508 Acc: 1.0000\n",
      "Train Epoch: 9 [56000/60000] Loss: 0.280275 Acc: 0.8750\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.177223 Acc: 1.0000\n",
      "Train Epoch: 9 [59200/60000] Loss: 0.245070 Acc: 0.8750\n",
      "Elapsed 145.11s, 14.51 s/epoch, 0.00 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2883, Accuracy: 8950/10000 (90%)\n",
      "\n",
      "Total time: 146.70, Best Loss: 0.281\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "modelBN = LeNetBN() \n",
    "\n",
    "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
    "\n",
    "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">10. Plot Loss</font> <a name=\"plot-loss\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
    "\n",
    "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
    "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
    "\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above curves show that when we use Batch Normalization, the training converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">Miscellaneous: Calculate Mean and Standard Deviation of Fashion MNIST </font> <a name=\"misc\"></a>\n",
    "\n",
    "Ideally, we should not use the same mean and standard deviation for Fashion MNIST and MNIST. Refrain, even when you find many continuing to do this, simply because it does not have a profound effect on the results.\n",
    "\n",
    "Let us find  the mean and standard deviation for Fashion MNIST and use it instead of MNIST.\n",
    "\n",
    "We need to simply find the mean and standard deviation of the whole dataset. So, we load the dataset and then use the functions given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([60000, 28, 28])\n",
      "tensor(0.2860)\n",
      "tensor(0.3530)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(root=\"../../../../data/Fasion_MNIST/\", train=True, download=False, transform=train_transform)\n",
    "print(type(train_set.data))\n",
    "print(train_set.data.shape)\n",
    "print(train_set.data.float().mean()/255)\n",
    "print(train_set.data.float().std()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
