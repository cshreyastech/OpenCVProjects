{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">Assignment: Implement a CNN for Image Classification on CIFAR10 dataset</font>\n",
    "\n",
    "We have seen how to implement a CNN (LeNet5 and LeNet with the batch norm) in the last section. We used MNIST and Fashion MNIST dataset which are grayscale or single channel datasets. In this assignment, you will implement a CNN Model ( similar to LeNet ) for classifying objects in the `CIFAR10` dataset. \n",
    "\n",
    "The CIFAR10 dataset has the following properties\n",
    "1. It has `10` classes.  \n",
    "1. It has colored images, so it has `3-channels`. \n",
    "1. The image shape is `32 x 32`.\n",
    "\n",
    "Samples of CIFAR10- dataset ([source](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html?highlight=cifar)):\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w3_cirar10.png\" width=700>\n",
    "\n",
    "\n",
    "# <font color='blue'>Marking Scheme</font>\n",
    "\n",
    "### <font style=\"color:green\">Maximum Points: 30\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Problem</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Implement the CNN Model</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Find Mean and Std of Training Data</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Model Training & Accuracy</h3></td> <td><h3>15</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n",
    "\n",
    "\n",
    "# <font color='blue'>Problem Description</font>\n",
    "\n",
    "### <font color='blue'>1. Implement the CNN Model</font>\n",
    "Since the task is to classify objects in a dataset of color images, you need to implement a CNN with 10 output classes. **Also, your model must use `Conv2d`, `BatchNorm2d`, and `ReLU`.** \n",
    "\n",
    "**You need to define the model architecture in the function: `MyModel` ( Step 1 )**\n",
    "\n",
    "Hint: For color images you need to use an input shape that is different than the ones we have been using till now, so that it accepts 3 channel inputs.\n",
    "\n",
    "### <font color='blue'>2. Find Mean and Std of Training Data</font>\n",
    "\n",
    "It is a good practice to normalize the training data. To normalize the data, we need to compute mean and std. As the dataset has colored images, it has `3-channel` (RGB or BGR). We have to find mean and std per channel using training data. \n",
    "\n",
    "**You need to compute the mean and std for the dataset in the function: `get_mean_std_train_data` ( Step 3 )**\n",
    "\n",
    "### <font color='blue'>3. Model Training and Accuracy</font>\n",
    "\n",
    "Once you have defined the model, you can train it. To get better accuracy, you need to play around the training configuration **( Step 5 )** and even the model architecture. You can check the accuracy by running the training loop in `Step 11`.\n",
    "\n",
    "Here are a few hints on how you can improve the accuracy:\n",
    "- Train for longer duration\n",
    "- Try with different learning rate\n",
    "- Try to add more convolutional layers to the architecture\n",
    "- Try to add more nodes in the layers.\n",
    "\n",
    "You need to achieve **75% accuracy** ( See Step11 ) in order to get full marks for this part. \n",
    "\n",
    "**You do not need to implement anything for this, just changing the parameters as mentioned above and running the Notebook will give you the accuracy. ( Step 5 and Step 11 )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Note that this notebook requires you to change a few stuff in the model to get the desired accuracy. Therefore you need train the model all over again (which seems to be time-consuming).   \n",
    "\n",
    "You can choose to execute this notebook in Google-Colab so that you have access to a GPU-machine and prototype faster.   \n",
    "\n",
    "Once the desired results are acheived, you can copy-paste the changes made in the Colab-notebook to this notebook so that the grading occurs on the latest code. \n",
    "\n",
    "You can access the Colab-notebook from [here](https://colab.research.google.com/drive/18lgSRmHPagJkB0xmDq5ZiWGkuWcHnUc2?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "required_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">1. CNN Model Architecture [10 Points]</font>\n",
    "\n",
    "You have to write the model code here. You can take reference from LeNet code.\n",
    "\n",
    "If you do not get higher accuracy, here are a few hints:\n",
    "- Try to add more convolutional layers to the architecture\n",
    "- Try to add more nodes in the layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         ###\n",
    "#         ### YOUR CODE HERE\n",
    "#         ###\n",
    "#         # convolution layers\n",
    "#         self._body = nn.Sequential(\n",
    "#             # First convolution Layer\n",
    "#             # n_out = floor((n_in + 2p - k) / s) + 1\n",
    "            \n",
    "#             # n_out = floor(((32 + 2 * 0 - 5) / 1) + 1 = 28\n",
    "#             # input size = (32, 32), output size = (28, 28)\n",
    "#             nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
    "#             nn.BatchNorm2d(6),\n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Max pool 2-d\n",
    "#             # n_out = floor(((28 + 2 * 0 - 2) / 2) + 1 = 14\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "#             # Second convolution layer\n",
    "#             # input size = (14, 14), output size = (10, 10)\n",
    "#             # n_out = floor(((14 + 2 * 0 - 5) / 1) + 1 = 10\n",
    "#             nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # n_out = floor(((10 + 2 * 0 - 2) / 2) + 1 = 5\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "#             # output size = (5, 5)\n",
    "#         )\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         self._head = nn.Sequential(\n",
    "#             # First fully connected layer\n",
    "#             # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "#             nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # second fully connected layer\n",
    "#             # in_features = output of last linear layer = 120 \n",
    "#             nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Third fully connected layer which is also output layer\n",
    "#             # in_features = output of last linear layer = 84\n",
    "#             # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "#             nn.Linear(in_features=84, out_features=10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # apply feature extractor\n",
    "#         x = self._body(x)\n",
    "#         print(\"x shape1 \", x.shape)\n",
    "#         # flatten the output of conv layers\n",
    "#         # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "#         x = x.view(x.size()[0], -1)\n",
    "#         print(\"x shape2 \", x.shape)\n",
    "#         # apply classification head\n",
    "#         x = self._head(x)\n",
    "#         print(\"x shape3 \", x.shape)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         ###\n",
    "#         ### YOUR CODE HERE\n",
    "#         ###\n",
    "#         # convolution layers\n",
    "#         self._body = nn.Sequential(\n",
    "#             # First convolution Layer\n",
    "#             # n_out = floor((n_in + 2p - k) / s) + 1\n",
    "            \n",
    "#             # n_out = floor(((32 + 2 * 0 - 3) / 1) + 1 = 30\n",
    "#             # input size = (32, 32), output size = (30, 30)\n",
    "#             nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3),\n",
    "#             nn.BatchNorm2d(6),\n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Max pool 2-d\n",
    "#             # n_out = floor(((30 + 2 * 0 - 2) / 2) + 1 = 15\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "#             # Second convolution layer\n",
    "#             # input size = (15, 15), output size = (10, 10)\n",
    "#             # n_out = floor(((15 + 2 * 0 - 3) / 1) + 1 = 13\n",
    "#             nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # n_out = floor(((13 + 2 * 0 - 2) / 2) + 1 = 6\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "#             # output size = (6, 6)\n",
    "\n",
    "#             # Third convolution layer\n",
    "#             # input size = (5, 5), output size = (10, 10)\n",
    "#             # n_out = floor(((5 + 2 * 0 - 3) / 1) + 1 = 3\n",
    "#             # nn.Conv2d(in_channels=5, out_channels=32, kernel_size=3),\n",
    "#             # nn.BatchNorm2d(32),\n",
    "#             # nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # n_out = floor(((3 + 2 * 0 - 2) / 2) + 1 = 5\n",
    "#             # nn.MaxPool2d(kernel_size=2),\n",
    "#             # output size = (5, 5)\n",
    "#         )\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         self._head = nn.Sequential(\n",
    "#             # First fully connected layer\n",
    "#             # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "#             nn.Linear(in_features=16 * 6 * 6, out_features=120), \n",
    "            \n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # second fully connected layer\n",
    "#             # in_features = output of last linear layer = 120 \n",
    "#             nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Third fully connected layer which is also output layer\n",
    "#             # in_features = output of last linear layer = 84\n",
    "#             # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "#             nn.Linear(in_features=84, out_features=10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # apply feature extractor\n",
    "#         x = self._body(x)\n",
    "#         print(\"x shape1 \", x.shape)\n",
    "#         # flatten the output of conv layers\n",
    "#         # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "#         x = x.view(x.size()[0], -1)\n",
    "#         print(\"x shape2 \", x.shape)\n",
    "#         # apply classification head\n",
    "#         x = self._head(x)\n",
    "#         print(\"x shape3 \", x.shape)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         ###\n",
    "#         ### YOUR CODE HERE\n",
    "#         ###\n",
    "#         # convolution layers\n",
    "#         self._body = nn.Sequential(\n",
    "#             # First convolution Layer\n",
    "#             # n_out = floor((n_in + 2p - k) / s) + 1\n",
    "            \n",
    "#             # n_out = floor(((32 + 2 * 0 - 3) / 1) + 1 = 30\n",
    "#             # input size = (32, 32), output size = (30, 30)\n",
    "#             nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3),\n",
    "#             nn.BatchNorm2d(6),\n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.25),\n",
    "#             # Max pool 2-d\n",
    "#             # n_out = floor(((30 + 2 * 0 - 2) / 2) + 1 = 15\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "#             # Second convolution layer\n",
    "#             # input size = (15, 15), output size = (10, 10)\n",
    "#             # n_out = floor(((15 + 2 * 0 - 3) / 1) + 1 = 13\n",
    "#             nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.25),\n",
    "            \n",
    "#             # n_out = floor(((13 + 2 * 0 - 2) / 2) + 1 = 6\n",
    "#             # nn.MaxPool2d(kernel_size=2),\n",
    "#             # output size = (6, 6)\n",
    "\n",
    "#             # Third convolution layer\n",
    "#             # input size = (5, 5), output size = (10, 10)\n",
    "#             # n_out = floor(((13 + 2 * 0 - 3) / 1) + 1 = 11\n",
    "#             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.25),\n",
    "            \n",
    "#             # n_out = floor(((11 + 2 * 0 - 2) / 2) + 1 = 5\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "#             # output size = (5, 5)\n",
    "#         )\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         self._head = nn.Sequential(\n",
    "#             # First fully connected layer\n",
    "#             # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "#             nn.Linear(in_features=32 * 5 * 5, out_features=120), \n",
    "            \n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # second fully connected layer\n",
    "#             # in_features = output of last linear layer = 120 \n",
    "#             nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             nn.Linear(in_features=84, out_features=50), \n",
    "            \n",
    "#             # ReLU activation\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Third fully connected layer which is also output layer\n",
    "#             # in_features = output of last linear layer = 84\n",
    "#             # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "#             nn.Linear(in_features=50, out_features=10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # apply feature extractor\n",
    "#         x = self._body(x)\n",
    "        \n",
    "#         # print(\"x shape1 \", x.shape)\n",
    "#         # flatten the output of conv layers\n",
    "#         # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "#         x = x.view(x.size()[0], -1)\n",
    "#         # print(\"x shape2 \", x.shape)\n",
    "#         # apply classification head\n",
    "#         x = self._head(x)\n",
    "#         # print(\"x shape3 \", x.shape)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         ###\n",
    "#         ### YOUR CODE HERE\n",
    "#         ###\n",
    "\n",
    "#         # convolutional layer (sees 32x32x3 image tensor)\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "#         # convolutional layer (sees 16x16x16 tensor)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "#         # convolutional layer (sees 8x8x32 tensor)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "#         self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "#         # max pooling layer\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         # linear layer (64 * 4 * 4 -> 500)\n",
    "#         self.fc1 = nn.Linear(64 * 4 * 4, 500)\n",
    "#         # linear layer (500 -> 10)\n",
    "#         self.fc2 = nn.Linear(500, 10)\n",
    "#         # dropout layer (p=0.25)\n",
    "#         self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # add sequence of convolutional and max pooling layers\n",
    "#         # n_out = floor((n_in + 2 * p - k) / s) + 1\n",
    "#         # n_out = floor((32 + 2 * 0 - 2) / 2) + 1 = 16\n",
    "#         x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "#         # n_out = floor((16 + 2 * 0 - 2) / 2) + 1 = 8\n",
    "#         x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "#         # n_out = floor((8 + 2 * 0 - 2) / 2) + 1 = 4\n",
    "#         x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "#         # flatten image input\n",
    "#         x = x.view(-1, 64 * 4 * 4)\n",
    "#         # add dropout layer\n",
    "#         x = self.dropout(x)\n",
    "#         # add 1st hidden layer, with relu activation function\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         # add dropout layer\n",
    "#         x = self.dropout(x)\n",
    "#         # add 2nd hidden layer, with relu activation function\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ###\n",
    "        ### YOUR CODE HERE\n",
    "        ###\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # n_out = floor((n_in + 2p - k) / s) + 1\n",
    "            \n",
    "            # n_out = floor(((32 + 2 * 0 - 3) / 1) + 1 = 30\n",
    "            # input size = (32, 32), output size = (30, 30)\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(p=0.25),\n",
    "            # Max pool 2-d\n",
    "            # n_out = floor(((30 + 2 * 0 - 2) / 2) + 1 = 15\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (15, 15), output size = (10, 10)\n",
    "            # n_out = floor(((15 + 2 * 0 - 3) / 1) + 1 = 13\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(p=0.25),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # n_out = floor(((13 + 2 * 0 - 2) / 2) + 1 = 6\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (6, 6)\n",
    "\n",
    "            # Third convolution layer\n",
    "            # input size = (5, 5), output size = (10, 10)\n",
    "            # n_out = floor(((13 + 2 * 0 - 3) / 1) + 1 = 11\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(p=0.25),\n",
    "            \n",
    "            # n_out = floor(((11 + 2 * 0 - 2) / 2) + 1 = 5\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=64 * 4 * 4, out_features=500), \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.25),\n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=500, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        \n",
    "        # print(\"x shape1 \", x.shape)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # print(\"x shape2 \", x.shape)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        # print(\"x shape3 \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">2. Display the Network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=500, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_model = MyModel()\n",
    "print(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Conv2d",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "BatchNorm2d",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ReLU",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "input-output",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">3. Find Mean and STD of CIFAR10 Data [5 Points]</font>\n",
    "\n",
    "Function **`get_mean_std_train_data`** should `return` `mean` and `std` of training data. You can refer to the code used in the previous section for finding the mean and std of the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_std_train_data(data_root):\n",
    "    \n",
    "    train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_set = datasets.CIFAR10(root=data_root, train=True, download=False, transform=train_transform)\n",
    "    \n",
    "    # return mean (numpy.ndarray) and std (numpy.ndarray)\n",
    "    # mean = np.array([0.5, 0.5, 0.5])\n",
    "    # std  = np.array([0.5, 0.5, 0.5])\n",
    "    \n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    mean = np.mean(train_set.data, axis=(0, 1, 2)) / 255\n",
    "    std = np.std(train_set.data, axis=(0, 1, 2)) / 255\n",
    "    return mean, std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "grade": true,
     "grade_id": "mean",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "grade": true,
     "grade_id": "std",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root, num_workers=1):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        mean, std = get_mean_std_train_data(data_root)\n",
    "        assert len(mean) == len(std) == 3\n",
    "    except:\n",
    "        mean = np.array([0.5, 0.5, 0.5])\n",
    "        std = np.array([0.5, 0.5, 0.5])\n",
    "        \n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean and divide by variance.\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root=data_root, train=True, download=False, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root=data_root, train=False, download=False, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root, num_workers=1):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        mean, std = get_mean_std_train_data(data_root)\n",
    "        assert len(mean) == len(std) == 3\n",
    "    except:\n",
    "        mean = np.array([0.5, 0.5, 0.5])\n",
    "        std = np.array([0.5, 0.5, 0.5])\n",
    "        \n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # this re-scale image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean and divide by variance.\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root=data_root, train=True, download=False, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root=data_root, train=False, download=False, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">4. System Configuration</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">5. Training Configuration [15 Points]</font>\n",
    "All training parameters are defined here. So, \n",
    "This is where you can improve your accuracy, apart from improving the architecture. \n",
    "\n",
    "Here are a few hints on how you can improve the accuracy:\n",
    "- Train for longer duration\n",
    "- Try with different learning rate\n",
    "\n",
    "**You need to achieve 75% accuracy in order to get full marks for this part.**\n",
    "\n",
    "**You will see the effect of these changes when you run Step 11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 30  # number of times the whole dataset will be passed through the network\n",
    "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
    "        \n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "\n",
    "    # \"../resource/lib/publicdata/images\"  # folder to save data\n",
    "    data_root: str = \"../../../../data/resource/lib/publicdata/images\"  # folder to save data\n",
    "    num_workers: int = 10  # number of concurrent processes using to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    # update changed parameters in blow coding block.\n",
    "    # Please do not change \"data_root\" \n",
    "    \n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">6. System Setup</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">7. Training</font>\n",
    "We are familiar with the training pipeline used in PyTorch. The following steps are performed in the code below:\n",
    "\n",
    "1. Send the data to the required device ( CPU/GPU )\n",
    "1. Make a forward pass using the forward method.\n",
    "1. Find the loss using the Cross_Entropy function.\n",
    "1. Find the gradients using the backward function.\n",
    "1. Update the weights using the optimizer.\n",
    "1. Find the accuracy of the model\n",
    "\n",
    "Repeat the above for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mood\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gardients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">8. Validation</font>\n",
    "\n",
    "After every few epochs **`validation`** will be called with the `trained model` and `test_loader` to get validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        output = model(data)\n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">9. Saving the Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, device, model_dir='models', model_file_name='cifar10_cnn_model.pt'):\n",
    "    \n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # make sure you transfer the model to cpu.\n",
    "    if device == 'cuda':\n",
    "        model.to('cpu')\n",
    "\n",
    "    # save the state_dict\n",
    "    if int(torch.__version__.split('.')[1]) >= 6:\n",
    "        torch.save(model.state_dict(), model_path, _use_new_zipfile_serialization=False)\n",
    "    \n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        model.to('cuda')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">10. Main</font>\n",
    "\n",
    "In this section of code, we use the configuration parameters defined above and start the training. Here are the important actions being taken in the code below:\n",
    "\n",
    "1. Set up system parameters like CPU/GPU, number of threads etc\n",
    "1. Load the data using dataloaders\n",
    "1. Create an instance of the LeNet model\n",
    "1. Specify optimizer to use.\n",
    "1. Set up variables to track loss and accuracy and start training.\n",
    "1. If loss decreases, saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def main(system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lowers batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        num_workers_to_set = 2\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=training_configuration.batch_size,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "\n",
    "    # initiate model\n",
    "    model = MyModel()\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_configuration.learning_rate\n",
    "    )\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    best_accuracy = torch.tensor(0)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "            \n",
    "            if current_accuracy > best_accuracy:\n",
    "                best_accuracy = current_accuracy\n",
    "                print('Accuracy improved, saving the model.\\n')\n",
    "                save_model(model, device)\n",
    "            \n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}, Best Accuracy: {:.3f}\".format(time.time() - t_begin, best_loss, \n",
    "                                                                                best_accuracy))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Accuracy",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 11: Start Training</font>\n",
    "This is where you start the training. You may see that the training does not converge or does not give a good accuracy. You need to change \n",
    "- In Step 1: the network architecture and add a few more layers or more nodes to the already existing layers\n",
    "- In Step 5: training parameters such as learning rate or batch_size or epochs so that the network converges or run the network for longer so that it gets more time to fit the data\n",
    "\n",
    "**You need to make sure that the accuracy at the end is at least 75%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_training:\n\u001b[0;32m----> 2\u001b[0m     model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(system_configuration, training_configuration)\u001b[0m\n\u001b[1;32m     19\u001b[0m     num_workers_to_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# data loader\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_configuration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_configuration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers_to_set\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Update training configuration\u001b[39;00m\n\u001b[1;32m     29\u001b[0m training_configuration \u001b[38;5;241m=\u001b[39m TrainingConfiguration(\n\u001b[1;32m     30\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     31\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers_to_set\n\u001b[1;32m     32\u001b[0m )\n",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(batch_size, data_root, num_workers)\u001b[0m\n\u001b[1;32m     12\u001b[0m train_test_transforms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# this re-scale image tensor values between 0-1. image_tensor /= 255\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# subtract mean and divide by variance.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean, std)\n\u001b[1;32m     17\u001b[0m ])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# train dataloader\u001b[39;00m\n\u001b[1;32m     20\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_test_transforms\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     22\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     23\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# test dataloader\u001b[39;00m\n\u001b[1;32m     28\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     29\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39mdata_root, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtrain_test_transforms),\n\u001b[1;32m     30\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     31\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers\n\u001b[1;32m     33\u001b[0m )\n",
      "File \u001b[0;32m~/virtualenvs/pytorch_env/lib/python3.12/site-packages/torchvision/datasets/cifar.py:69\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[1;32m     72\u001b[0m     downloaded_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_list\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "if required_training:\n",
    "    model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">12. Plot Loss</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_loss, color='r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, color='b', label=\"validation loss\")\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">13. Plot Accuracy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_acc, color='r', label=\"train accuracy\")\n",
    "plt.plot(x, epoch_test_acc, color='b', label=\"validation accuracy\")\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='center right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">14. Loading the Model </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "cnn_model = MyModel()\n",
    "\n",
    "models = 'models'\n",
    "\n",
    "model_file_name = 'cifar10_cnn_model.pt'\n",
    "\n",
    "model_path = os.path.join(models, model_file_name)\n",
    "\n",
    "# loading the model and getting model parameters by using load_state_dict\n",
    "cnn_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">15. Model Prediction</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def prediction(model, train_config, batch_input):\n",
    "    \n",
    "    # send model to cpu/cuda according to your system configuration\n",
    "    model.to(train_config.device)\n",
    "    \n",
    "    # it is important to do model.eval() before prediction\n",
    "    model.eval()\n",
    "\n",
    "    data = batch_input.to(train_config.device)\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # Score to probability using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "\n",
    "    # get the max probability\n",
    "    pred_prob = prob.data.max(dim=1)[0]\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">16. Perform Inference on sample images </font>\n",
    "\n",
    "For prediction, we need to transform the data in the same way as we have done during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "batch_size = 5\n",
    "train_config = TrainingConfiguration()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_config.device = \"cuda\"\n",
    "else:\n",
    "    train_config.device = \"cpu\"\n",
    "    \n",
    "    \n",
    "\n",
    "# load test data without image transformation\n",
    "test = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root=train_config.data_root, train=False, download=False, \n",
    "                   transform=transforms.functional.to_tensor),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1\n",
    "    )\n",
    "\n",
    "try:\n",
    "    mean, std = get_mean_std_train_data(data_root)\n",
    "    assert len(mean) == len(std) == 3\n",
    "except:\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "\n",
    "# load testdata with image transformation\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "test_trans = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root=train_config.data_root, train=False, download=False, transform=image_transforms),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1\n",
    "    )\n",
    "\n",
    "for data, _ in test_trans:\n",
    "    # pass the loaded model\n",
    "    pred, prob = prediction(cnn_model, train_config, data)\n",
    "    break\n",
    "    \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (3, 3)\n",
    "for images, label in test:\n",
    "    for i, img in enumerate(images):\n",
    "        img = transforms.functional.to_pil_image(img)\n",
    "        plt.imshow(img)\n",
    "        plt.gca().set_title('Pred: {0}({1:0.2}), Label: {2}'.format(classes[pred[i]], prob[i], classes[label[i]]))\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">17. Report your findings</font>\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">References</font>\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "1. https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
