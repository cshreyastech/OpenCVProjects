{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Binary Classification using a Perceptron</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the building blocks of a Neural Network. Before we start going into the details of neural networks, we will illustrate what a single neuron is capable of! Specifically, we will solve a Binary Classification problem ( with the same linearly separable data shown in the earlier lecture ) using a single Neuron.\n",
    "\n",
    "The insteresting part is that we will implement all the building blocks from scratch so that you understand all the nuts and bolts of a Neuron and Neural Networks(by extension)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(8,133,37)\">Problem statement</font>\n",
    "\n",
    "We have two sets of data points generated with different mean and variance. The task is to train a classifier which is able to distinguish between the two classes. In other words, We have to find a hyperplane that separates the two sets of data. \n",
    "\n",
    "We have chosen the two-dimensional data as shown below, so we have to learn a line that separates these two sets of points. We will see in this notebook that by training the perceptron, we are trying to learn the parameters of the network which is same as estimating the parameters of the line separating the classes (also known as the **decision boundary**)\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w2_problem_statement.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">1. Training Pipeline</font>\n",
    "Given below is the training pipeline which consists of the Neuron and the weight update loop using the gradients. We have already learnt about neurons, activation functions, gradients and gradient descent weight update process in the previous sections. It's now time to put them all together and train a Binary Classifier.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w2_single_neuron_train_arch.jpg\" width=1000>\n",
    "\n",
    "Let us understand the above pipeline in more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">The Neuron</font>\n",
    "The first part is the **`Neuron`** itself. The neuron has the following architecture:\n",
    "\n",
    "1. Two `inputs` : $X = [x_1, x_2]$\n",
    "1. Two `weights` : $W = [w_1, w_2]$\n",
    "1. A `bias` Term : $B = b$\n",
    "1. A `summation` which performs a weighted addition of the inputs. **The weights are learnable parameters**. Let's call it z for the time-being.\n",
    "\n",
    "$$w_1x_1 + w_2x_2 + b = z(say)$$\n",
    "\n",
    "5. A `sigmoid activation` function at the output of the neuron. Mathematically, it is given as\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-(w_1x_1 + w_2x_2 + b)}} = y'(say)\n",
    "$$\n",
    "\n",
    "## <font style=\"color:green\">Loss Function</font>\n",
    "$y'$ gives the predicted output for a given input. Since this is the output of a sigmoid, it can also be thought of as a probability measure of class 1 or in other words, it gives the likelihood of class 1. We compare y' with the actual output ( also called thr ground_truth ) and compute a loss function. The loss function used for binary classification is the **`Binary Cross Entropy loss function`**. We will discuss about this in the next section. Mathematically, it is given as:\n",
    "\n",
    "$$\n",
    "J(y') = -y\\log(y') - (1-y)\\log(1-y')\n",
    "$$\n",
    "\n",
    "## <font style=\"color:green\">Backpropagation</font>\n",
    "The final part is the Weight update where we use **`Backpropagation`** to compute gradients and update the weights.\n",
    "\n",
    "\n",
    "**Compute Gradients**\n",
    "\n",
    "We need to compute the gradients of the loss function with respect to the weights and biases. \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W}, \\frac{\\partial J}{\\partial B}\n",
    "$$\n",
    "\n",
    "**Chain Rule**\n",
    "\n",
    "Since we cannot compute the gradients directly, we can use the chain rule described in the last section on Backpropagation. Applying chain rule, we get the following:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w_1} &= \\frac{\\partial J}{\\partial y'}.\\frac{\\partial y'}{\\partial z}. \\frac{\\partial z}{\\partial w_1}  \\\\\n",
    "\\frac{\\partial J}{\\partial w_2} &= \\frac{\\partial J}{\\partial y'}.\\frac{\\partial y'}{\\partial z}. \\frac{\\partial z}{\\partial w_2}  \\\\\n",
    "\\frac{\\partial J}{\\partial b} &= \\frac{\\partial J}{\\partial y'}.\\frac{\\partial y'}{\\partial z}. \\frac{\\partial z}{\\partial b}  \\\\\n",
    "\\end{align}\n",
    "\n",
    "**NOTE**: We can calculate the above derivatives separately and multiply them to get the required gradient.\n",
    "\n",
    "**Weight Update**\n",
    "\n",
    "The final step is to update the weights using the calculated gradients. This is done using the following equations.\n",
    "\n",
    "$$\n",
    "W \\leftarrow W - \\gamma \\frac{\\partial J}{\\partial W}, \\\n",
    "B \\leftarrow B - \\gamma \\frac{\\partial J}{\\partial B}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python\n",
    "import matplotlib.animation as animation\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reproducibility first - let's fix all the random numbers generators to obtain reproducible results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">2. Create Data points</font>\n",
    "\n",
    "We will create random data using a gaussian distribution. The data is similar to the one discussed in the lecture. This will be used for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_data_points = 500\n",
    "class_zero_points = torch.empty(num_data_points, 2).normal_(mean=2,std=0.5)\n",
    "class_one_points = torch.empty(num_data_points, 2).normal_(mean=4,std=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(class_zero_points[:,0], class_zero_points[:,1], s=8, color='b', label='Class:0')\n",
    "plt.scatter(class_one_points[:,0], class_one_points[:,1], s=8, color='r', label='Class:1')\n",
    "plt.legend()\n",
    "plt.xlim([-1, 8])\n",
    "plt.ylim([-1, 8])\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">3. Prepare Data</font>\n",
    "\n",
    "We prepare the data so that it contains the data points from both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_zero = torch.zeros_like(class_zero_points[:,0], dtype=int)\n",
    "label_one = torch.ones_like(class_one_points[:,0], dtype=int)\n",
    "\n",
    "label = torch.cat([label_zero, label_one])\n",
    "data_points = torch.cat([class_zero_points, class_one_points], dim=0)\n",
    "\n",
    "print('Data points size: {}'.format(data_points.size()))\n",
    "print('Label size: {}'.format(label.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">4. Implementing the Perceptron</font>\n",
    "This is the neuron which is simply calculating the weighted sum of the inputs. It is given by:\n",
    "\n",
    "$$\n",
    "WX + B = w_1x_1 + w_2x_2 + b = z \\text{  (let's)}\n",
    "$$\n",
    "\n",
    "Let's calculate the derivative of $z$ w.r.t the weights and bias.\n",
    "\n",
    "**Derivative of $WX + B$  w.r.t its input $W$ and $B$:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial w_1} &= x_1  \\\\\n",
    "\\frac{\\partial z}{\\partial w_2} &= x_2  \\\\\n",
    "\\frac{\\partial z}{\\partial b} &= 1 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's define the perceptron and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Neuron: WX + B\n",
    "def wx_plus_b(W, X, B):\n",
    "    \n",
    "    return torch.matmul(X, W) + B\n",
    "\n",
    "# Derivative of WX + B w.r.t its input W and B\n",
    "def grad_wx_plus_b(X):\n",
    "    batch_size = X.size(0)\n",
    "    g_w = X\n",
    "    g_b = torch.ones(batch_size)\n",
    "    \n",
    "    return g_w, g_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">5. Why Sigmoid Activation?</font>\n",
    "The Sigmoid function is given by:\n",
    "\n",
    "$$\n",
    "y' = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "Let's plot the curve of a sigmoid function and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sigmoid\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "z = torch.linspace(-10, 10, 1000)\n",
    "y = torch.sigmoid(z)\n",
    "\n",
    "plt.figure\n",
    "plt.plot(z, y, color='b')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid Activation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can observe the following points in `sigmoid` plot:**\n",
    "\n",
    "- $sigmoid(0) = 0.5$\n",
    "- $sigmoid(z) > 0.5$    $\\text{  }\\forall\\text{ }z > 0$\n",
    "- $sigmoid(z) < 0.5$    $\\text{  }\\forall\\text{ }z < 0$\n",
    "\n",
    "**We can use this property to derive the probability as follows:**\n",
    "\n",
    "- If $sigmoid(z) > 0.5$ then input belongs to the positive class or class `1`\n",
    "- If $sigmoid(z) < 0.5$ then input belongs to the negative class or class `0`\n",
    "\n",
    "The `sigmoid` output $y'$ may be thought of as probability that the data point belongs to class `1`. So, the probability that it belongs to class `0` will be $1-y'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(8,133,37)\">Implementing Sigmoid Activation</font>\n",
    "Let us implement the sigmoid activation and it's gradient function.\n",
    "\n",
    "**Sigmoid:**\n",
    "\n",
    "$$\n",
    "y' = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Derivative of sigmoid w.r.t. its input.**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y'}{\\partial z}  = \\sigma(z)(1 - \\sigma(z)) = y'(1-y')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "\n",
    "    return torch.sigmoid(x).squeeze()\n",
    "\n",
    "# Derivative of sigmoid w.r.t. its input.\n",
    "def grad_sigmoid(x):\n",
    "    \n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">6. Why Binary Cross-Entropy?</font>\n",
    "The Binary cross entropy is given by:\n",
    "$$\n",
    "J(y^{'}) = -y\\log(y^{'}) - (1-y)\\log(1-y{'})\n",
    "$$\n",
    "\n",
    "**Let's break down the equation since there are only two classes [0,1]:**\n",
    "\n",
    "\\begin{equation}\n",
    "J(y^{'})=\\left\\{\\begin{array}{cc} -\\log(y^{'}) & y=1\\\\ -\\log(1-y{'}) & y=0 \\end{array} \\right. \\label{eq2}\n",
    "\\end{equation} \n",
    "\n",
    "**Let's plot for class `1` and `0`.**\n",
    "\n",
    "For sigmoid activation case, input to our binary cross-entropy loss will lie between `0-to-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy plot y =1 \n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "x = torch.linspace(0, 1, 1000)\n",
    "y1 = -torch.log(x)\n",
    "y0 = -torch.log(1-x)\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, y1, color='b', label=\"J(y') | y = 1\")\n",
    "plt.plot(x, y0, color='r', label=\"J(y') | y = 0\")\n",
    "plt.xlabel(\"y'\")\n",
    "plt.ylabel(\"J(y')\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.title('Binary Cross-Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say a data point belongs to class `1` <font style=\"color:blue\">( Blue Curve )</font>. If the output of the network is close to `0`, then from the above diagram we can see that the loss will be very high ( The blue curve goes to infinity ). Conversely, the loss is `0` if the output of the network is `1`.\n",
    "\n",
    "The same logic goes for the <font style=\"color:red\">red curve</font>.\n",
    "\n",
    "Thus, this loss function accurately captures the binary classification error - i.e. the error is high when the predicted output is different from the ground truth and vice-versa. Thus, it is the correct loss function for solving a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(8,133,37)\">Implementing Binary Cross-Entropy</font>\n",
    "Now that we know why we are using Binary Cross Entropy loss, lets implement the loss and also its gradient function\n",
    "\n",
    "**Binary cross-entropy:**\n",
    "$$\n",
    "J(y') = -y\\log(y') - (1-y)\\log(1-y')\n",
    "$$\n",
    "\n",
    "**Derivative of binary cross-entropy w.r.t its input $y'$.**\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial y'} = -\\frac{y}{y'} + \\frac{1-y}{1-y'}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Binary cross-entropy\n",
    "def bce_loss(sigmoid_pred, g_truth):\n",
    "    loss = - (1 - g_truth)* torch.log(1 - sigmoid_pred) - g_truth * torch.log(sigmoid_pred)\n",
    "    return loss\n",
    "\n",
    "# Derivative of binary cross-entropy w.r.t its input.\n",
    "def grad_bce_loss(sigmoid_pred, g_truth):\n",
    "    return - (g_truth * (1 / sigmoid_pred)) + ((1 - g_truth) * (1 / (1 - sigmoid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">7. Graph Implementation</font>\n",
    "Next, We will implement the network graph, forward pass and backward pass. **We find the gradients of the parameters $w_1$, $w_2$ and $b$, using the chain rule:** Thus, we compute the gradients for each node and multiply them to get the final gradients of the loss function with respect to the weights. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w_1} &= \\frac{\\partial J}{\\partial y^{'}}.\\frac{\\partial y^{'}}{\\partial z}. \\frac{\\partial z}{\\partial w_1}  \\\\\n",
    "\\frac{\\partial J}{\\partial w_2} &= \\frac{\\partial J}{\\partial y^{'}}.\\frac{\\partial y^{'}}{\\partial z}. \\frac{\\partial z}{\\partial w_2}  \\\\\n",
    "\\frac{\\partial J}{\\partial b} &= \\frac{\\partial J}{\\partial y^{'}}.\\frac{\\partial y^{'}}{\\partial z}. \\frac{\\partial z}{\\partial b}  \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BinaryClassifierGraph:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It is initializing the variable that will be updated in `forward` and `loss` function. \n",
    "        Storing these values will be used in `backward` function to get the gradient.\n",
    "        \"\"\"\n",
    "        \n",
    "        # default gardient is zero\n",
    "        self.w0_grad = 0\n",
    "        self.w1_grad = 0\n",
    "        self.b_grad = 0\n",
    "        \n",
    "        self.x_in = None\n",
    "        self.wx_plus_b_out = None\n",
    "        self.sigmoid_out = None\n",
    "        \n",
    "        self.bce_loss = None\n",
    "        \n",
    "        self.grad_bce_loss = None\n",
    "        \n",
    "        self.g_truth = None\n",
    "        \n",
    "    def forward(self, w, x, b):\n",
    "        # updated input value, it will be used in backward pass\n",
    "        self.x_in = x\n",
    "        self.b_in = b\n",
    "        self.w_in = w\n",
    "        \n",
    "        # Intermediate node with the weighted sum\n",
    "        self.wx_plus_b_out = wx_plus_b(w, x, b)\n",
    "        \n",
    "        # Output node after applying activation function\n",
    "        self.sigmoid_out = sigmoid(self.wx_plus_b_out)\n",
    "            \n",
    "        return self.sigmoid_out\n",
    "    \n",
    "    def loss(self, g_truth):\n",
    "        \n",
    "        self.g_truth = g_truth\n",
    "        \n",
    "        # Compute the binary cross entropy loss\n",
    "        self.bce_loss = bce_loss(self.sigmoid_out, g_truth)\n",
    "        return self.bce_loss.mean()\n",
    "    \n",
    "    def backward(self):\n",
    "        # Compute the gradients of Loss w.r.t neuron output (y')\n",
    "        d_bce_loss = grad_bce_loss(self.sigmoid_out, self.g_truth)\n",
    "        \n",
    "        # Compute the gradients of neuron output(y') w.r.t weighted sum(z)\n",
    "        d_sigmoid = grad_sigmoid(self.wx_plus_b_out)\n",
    "        \n",
    "        # Compute the gradients of weighted sum(z) w.r.t weights and bias\n",
    "        d_w, d_b = grad_wx_plus_b(self.x_in)\n",
    "        \n",
    "        # Using chain rule to find overall gradient of Loss w.r.t weights and bias\n",
    "        self.w0_grad = d_bce_loss * d_sigmoid * d_w[:,0]\n",
    "        self.w1_grad = d_bce_loss * d_sigmoid * d_w[:,1]\n",
    "        self.b_grad = d_bce_loss * d_sigmoid * d_b\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def gradients(self):\n",
    "        \n",
    "        w_grad = torch.tensor([[self.w0_grad.mean()], [self.w1_grad.mean()]])\n",
    "        b_grad = torch.tensor([self.b_grad.mean()])\n",
    "        \n",
    "        return w_grad, b_grad\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">8. Update Parameters</font>\n",
    "\n",
    "This is the gradient descent update step which we are already familiar with from previous week! Just to recap, we take the gradients and update the weights and biases. We use a scaling factor called the **Learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(w, b, dw, db, lr):\n",
    "    w = w - lr * dw\n",
    "    b = b - lr * db\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">9. Training Loop</font>\n",
    "\n",
    "Training loop is running for the given number of epochs where we do the following:\n",
    "\n",
    "1. Divide the data into batches\n",
    "1. For each batch, we compute the activations using the `forward` pass.\n",
    "1. Compute the loss using Binary Cross Entropy\n",
    "1. Compute the gradients using the backward function.\n",
    "1. Finally, update gradient using `gradient_descent_update` function.\n",
    "\n",
    "We keep track of the loss for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train(w, b, data_points, label, epochs=100, lr=0.01, batch_size=10):\n",
    "    \n",
    "    bc = BinaryClassifierGraph()\n",
    "    \n",
    "    # for storing loss of each batch\n",
    "    avg_train_loss = np.array([])\n",
    "    \n",
    "    updated_parms = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # for storing loss of each batch in current epoch\n",
    "        avg_loss = np.array([])\n",
    "        \n",
    "        num_baches = int(len(label)/batch_size)\n",
    "        \n",
    "        # Shuffle data and label\n",
    "        shuffled_index = random.sample(range(len(label)), len(label))  \n",
    "        s_data_points = data_points[shuffled_index]\n",
    "        s_label = label[shuffled_index]\n",
    "        \n",
    "        print('\\nEpoch: {}'.format(epoch+1))\n",
    "        \n",
    "        for batch_idx in range(num_baches):\n",
    "            # get training data in batch\n",
    "            start_index = batch_idx * batch_size\n",
    "            end_index = (batch_idx + 1) * batch_size\n",
    "            data = s_data_points[start_index:end_index]\n",
    "            g_truth = s_label[start_index:end_index]\n",
    "            \n",
    "            # forward pass\n",
    "            bc.forward(w, data, b)\n",
    "            \n",
    "            # Find loss\n",
    "            loss = bc.loss(g_truth)\n",
    "            \n",
    "            # Backward will find gradient using chain rule \n",
    "            bc.backward()\n",
    "            \n",
    "            # Get gradients after they are updated using the backward function\n",
    "            grad_w, grad_b = bc.gradients()\n",
    "            \n",
    "            # Update parameters using gradient descent\n",
    "            w, b = gradient_descent_update(w, b, grad_w, grad_b, lr)\n",
    "                \n",
    "            # to show training results\n",
    "            avg_loss = np.append(avg_loss, [loss])\n",
    "            avg_train_loss = np.append(avg_train_loss, [loss])\n",
    "            time.sleep(0.001)\n",
    "            print(\"\\rBatch: {0}/{1} | Avg Batch Loss: {2:.3} | Batch Loss: {3:.3} | Avg Train Loss:{4:.3}\".\n",
    "                  format(batch_idx+1, num_baches, avg_loss.mean(), loss.item(), avg_train_loss.mean()), end=\"\")\n",
    "    \n",
    "        # storing parameters to show decision boundary animition\n",
    "        updated_parms.append((w.data[0][0].clone(), w.data[1][0].clone(), b.data[0].clone()))\n",
    "            \n",
    "    return w, b, avg_train_loss, updated_parms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">10. Start Training</font>\n",
    "\n",
    "We initialize the parameters with random weights and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "w = torch.randn(input_size, 1)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "w, b, avg_train_loss, updated_parms= train(w, b, data_points, label)\n",
    "print('\\nw:\\n{}'.format(w))\n",
    "print('\\nb:\\n{}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">11. Plot the Decision Boundary</font>\n",
    "We will plot the decision boundary using the weight and biases obtained from training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(class_zero_points[:,0], class_zero_points[:,1], s=8, color='b', label='Class:0')\n",
    "plt.scatter(class_one_points[:,0], class_one_points[:,1], s=8, color='r', label='Class:1')\n",
    "x1 = torch.linspace(-1, 8, 1000)\n",
    "x2 = -(b.data[0] + w.data[0][0] * x1)/ w.data[1][0]\n",
    "plt.plot(x1, x2, c='g', label='Learned decision boundary:\\n{0:.2}x1 + {1:.2}x2 + {2:.2} = 0'.\n",
    "         format(w.data[0][0], w.data[1][0], b.data[0]))\n",
    "plt.legend()\n",
    "plt.xlim([-1, 8])\n",
    "plt.ylim([-1, 8])\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">12. Plot the Loss Curve</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.figure\n",
    "plt.plot(range(len(avg_train_loss)), avg_train_loss)\n",
    "plt.xlabel('Batch no.')\n",
    "plt.ylabel('Batch Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">13. Accuracy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(w, b, inputs, label):\n",
    "    bc = BinaryClassifierGraph()\n",
    "    prediction = bc.forward(w, inputs, b)\n",
    "    pred = prediction >= 0.5\n",
    "    pred = pred.squeeze()\n",
    "    label = label.type(torch.bool)\n",
    "    count = len(label)\n",
    "    \n",
    "    correct_pred = torch.sum(torch.eq(pred, label))\n",
    "    accuracy = correct_pred * 1.0/count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(w, b, data_points, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">14. Decision Boundary Animation</font>\n",
    "We have saved the weights at the end of each epoch. So, we can plot them and see how the weights got updated and how decision boundary changed before it converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim(-1, 8)\n",
    "ax.set_ylim(-1, 8)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "\n",
    "line, = ax.plot(0, 0, color='g', label='Decision Boundary')\n",
    "ax.scatter(class_zero_points[:,0], class_zero_points[:,1], s=8, color='b', label='Class:0')\n",
    "ax.scatter(class_one_points[:,0], class_one_points[:,1], s=8, color='r', label='Class:1')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "def plot_points_line(parm, line):\n",
    "    x1 = torch.linspace(-1, 8, 1000)\n",
    "    x2 = -(parm[2] + parm[0] * x1)/ parm[1]\n",
    "    line.set_xdata(x1)\n",
    "    line.set_ydata(x2)\n",
    "    \n",
    "    return line, \n",
    "    \n",
    "\n",
    "line_animation = animation.FuncAnimation(fig, func=plot_points_line, frames=updated_parms, fargs=(line,),\n",
    "                                         interval=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(8,133,37)\">Exercise</font>\n",
    "\n",
    "1. Try to use MSE loss and see if it converges to an optimal solution. If not, find the reason. Check if the vanishing gradient is the culprit.\n",
    "\n",
    "2. Instead of using the chain rule, derive the gradient function of the whole graph, and try using this for gradient update. For deriving the gradient function, you can use the chain rule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
