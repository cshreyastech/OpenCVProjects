{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTlg8FdNKqMq"
   },
   "source": [
    "# <font style=\"color:blue\">Image Classification using Multi Layer Perceptron</font>\n",
    "\n",
    "\n",
    "In this notebook, we will train a Multi-Layer Peceptron with two hidden layers to classify handwritten digits from the MNIST dataset.\n",
    "\n",
    "Let's first set import the standard libraries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mYuwGycSKqMr",
    "outputId": "75cf0278-1ec1-4494-e89b-d0be9c6a0f28"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get reproducible results\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnKMxkM-hvi-"
   },
   "source": [
    "## <font style=\"color:green\">Load Data</font>\n",
    "\n",
    "We will use the MNIST dataset. You will know more about this dataset, its history and importance in deep learning during the next lecture, for now you only need to know that MNIST is a handwritten digits dataset. It contains 60000 training and 10000 testing grayscale 28x28 images from 10 classes:\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w2_Mnist.png\" width=650>\n",
    "\n",
    "### <font style=\"color:green\">torch.dataset</font>\n",
    "We use the MNIST dataset that comes bundled with PyTorch. PyTorch provides easy access to some standard datasets using `torch.dataset`. Find list of supported data [here](https://pytorch.org/vision/stable/datasets.html).\n",
    "\n",
    "### <font style=\"color:green\">Download data and convert to PyTorch tensor</font>\n",
    "\n",
    "1. We load the training and validation data separately. \n",
    "2. We specify that the data should be downloaded if it is not present on the system.\n",
    "3. The data is transformed to PyTorch tensors. \n",
    "\n",
    "### <font style=\"color:green\">DataLoader</font>\n",
    "\n",
    "PyTorch provides a very useful class called `DataLoader` that helps feed the data during the training process. It is primarily used for two purposes. \n",
    "\n",
    "1. Load a mini-batch of data from a dataset. \n",
    "2. Shuffle the data (if required). \n",
    "\n",
    "**What batch size to use?**\n",
    "\n",
    "We are using a batch size of 32. When you are using a GPU, the maximum batch size is dictated by the memory on the GPU. However, even without the GPU memory limitation, batch size of 32 or smaller is preferred in many applications. See this [funny tweet](https://twitter.com/ylecun/status/989610208497360896?lang=en). \n",
    "\n",
    "**Why shuffle training set?**\n",
    "\n",
    "Notice in the code below, we shuffle the training data. This is because the original dataset may have some ordering (e.g. all examples of 0s come first, and then all 1s etc.). This kind of correlation is bad for the training process because the loss calculated over a mini-batch is used to update the weights or network parameters. On the other hand, it makes no sense to shuffle the validation set because validation loss is calculated over the entire validation set.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": [],
    "colab_type": "code",
    "id": "6ncSNeAKhd4T"
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "train_dataset = datasets.MNIST('resource/lib/publicdata/data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "# Validation dataset\n",
    "validation_dataset = datasets.MNIST('resource/lib/publicdata/data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "# Batch size : How many images are used to calculate the gradient\n",
    "batch_size = 32\n",
    "\n",
    "# Train DataLoader \n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "# Validation DataLoader \n",
    "validation_loader = DataLoader(dataset=validation_dataset, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpN6MG9Ahwe2"
   },
   "source": [
    "## <font style=\"color:green\">Create the Network</font>\n",
    "Here we define the multi layer perceptron. It has 2 hidden layers with 512 units. Also note that the input layer has 28x28 nodes which is the size of the flattened data. Given below is the schematic diagram of the network.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-mnist-schematic.jpg\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": [],
    "colab_type": "code",
    "collapsed": true,
    "id": "qLxFeFOAhd_d",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize super class\n",
    "        super().__init__()\n",
    "\n",
    "        # Build model using Sequential container\n",
    "        self.model = nn.Sequential(\n",
    "            # Add input layer \n",
    "            nn.Linear(28*28, 512),\n",
    "            # Add ReLU activation\n",
    "            nn.ReLU(),\n",
    "            # Add Another layer\n",
    "            nn.Linear(512, 512),\n",
    "            # Add ReLU activation\n",
    "            nn.ReLU(),\n",
    "            # Add Output layer\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDCb7nqIhxED"
   },
   "source": [
    "**Note** : For a classifier, the output layer should have **softmax** activation. However, we are using linear activation. This is because during training we will use `nn.CrossEntropyLoss` that combines `nn.LogSoftMax` (Log of SoftMax) and `nn.NLLLoss` (Negative Log Likelihood Loss). This also means that when we do inference, we have to use `nn.functional.softmax` on the raw output to convert it to probabilities. \n",
    "\n",
    "## <font style=\"color:green\">Train</font>\n",
    "\n",
    "This is the Training routine which does the following:\n",
    "\n",
    "1. It takes batches of data from train dataloader\n",
    "1. Prepares the input data in the form that can be fed to the network, i.e. it flattens the 28x28 image to a single 784 dimensional vector before passing it to the network.\n",
    "1. The training data is passed through the network\n",
    "1. Compute the cross entropy loss using the predicted output and the training labels\n",
    "1. Remove previous gradients using optimizer.zero_grad\n",
    "1. Compute Gradients using the backward function\n",
    "1. Update the weights using the optimizer.step function and repeat until all the data is passed through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": [],
    "colab_type": "code",
    "collapsed": true,
    "id": "pe5kTGjShX2D",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "\n",
    "    for (x_train, y_train) in train_loader:\n",
    "        \n",
    "        # Forward pass: \n",
    "        # Flatten the image since the input to the network is a 784 dimensional vector\n",
    "        x_train = x_train.view(x_train.shape[0], -1)\n",
    "        # Compute predicted y by passing x to the model\n",
    "        y = model(x_train)\n",
    "      \n",
    "        # Compute and print loss\n",
    "        loss = loss_function(y, y_train)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #Compute Accuracy\n",
    "        y_pred = y.argmax(dim=1)\n",
    "        correct = torch.sum(y_pred==y_train)\n",
    "        running_correct += correct\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate gradient using backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters (weights)\n",
    "        optimizer.step()\n",
    "  \n",
    "    return running_loss/len(train_loader), running_correct.item()/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_syMss5Ahx4l"
   },
   "source": [
    "## <font style=\"color:green\">Validate</font>\n",
    "\n",
    "We use the validation loader to pass batches of data through the network for performing validation on unseen data.\n",
    "\n",
    "Note that there is only forward pass and no backward pass during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": [],
    "colab_type": "code",
    "collapsed": true,
    "id": "omVfE6ndhX5G",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def val():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (x_val, y_val) in validation_loader:\n",
    "\n",
    "            # Forward pass: \n",
    "            # Flatten the image since the input to the network is a 784 dimensional vector\n",
    "            x_val = x_val.view(x_val.shape[0], -1)\n",
    "\n",
    "            # Compute raw score by passing x to the model\n",
    "            y = model(x_val)\n",
    "\n",
    "            # Score to probability using softmax\n",
    "            prob = nn.functional.softmax(y, dim=1)\n",
    "\n",
    "            #Compute Accuracy\n",
    "            y_pred = prob.argmax(dim=1)\n",
    "\n",
    "            correct = torch.sum(y_pred==y_val)\n",
    "            running_correct += correct\n",
    "\n",
    "            # Compute and print loss\n",
    "            loss = loss_function(y, y_val)\n",
    "            running_loss += loss.item()\n",
    "      \n",
    "    return running_loss/len(validation_loader), running_correct.item()/len(validation_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0yKh-N8hyfT"
   },
   "source": [
    "## <font style=\"color:green\">Configure Training Parameters</font>\n",
    "We first instantiate a MLP model using the MLP class defined above. We then specify the Cross Entropy loss for doing classification. This will be used for calculating the loss over each batch. Finally, we specify the optimizer which we have chosen to be SGD in this case with a Learning rate of 0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": [],
    "colab_type": "code",
    "collapsed": true,
    "id": "IPIRzpyyhP4H",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 20 \n",
    "\n",
    "# Construct model\n",
    "model = MLP()\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bp1936F5hzG7"
   },
   "source": [
    "\n",
    "Main Routine that calls the training and validation functions. We keep track of the loss of each epoch so that we can plot it to visualize the progressive change in loss over epochs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Lt41xnH7hP67",
    "outputId": "a1955ef3-3498-4650-c3a0-f8bd9a84fd86"
   },
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for ep in range(num_epochs):\n",
    "    train_loss, train_acc = train()\n",
    "    val_loss, val_acc = val()\n",
    "    print(\"Epoch: {}, Train Loss = {:.3f}, Train Acc = {:.3f} , Val Loss = {:.3f}, Val Acc = {:.3f}\".\n",
    "          format(ep, train_loss, train_acc, val_loss, val_acc))\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    val_acc_history.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ox1h02Lih0DD"
   },
   "source": [
    "### <font style=\"color:green\">Plot the Loss & Accuracy curves</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "id": "8qgFTBPYKsRd",
    "outputId": "8cc52335-74ad-454f-9054-513bdc80d5fe"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=[20,8])\n",
    "plt.subplot(121)\n",
    "plt.plot(train_loss_history,'r')\n",
    "plt.plot(val_loss_history,'b')\n",
    "plt.title(\"Loss Curve\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_acc_history,'r')\n",
    "plt.plot(val_acc_history,'b')\n",
    "plt.title(\"Accuracy Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSed7s25vae8"
   },
   "source": [
    "## <font style=\"color:green\">Perform Inference</font>\n",
    "\n",
    "We take a batch from the validation loader and pass it through the network to see if it gets classified correctly.\n",
    "\n",
    "First, we get the next batch from the validation loader. Then we plot the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "QV6XzOwSg6q1",
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    },
    "outputId": "7c961894-8e5b-4075-826d-baa8fcaf720f"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(validation_loader))\n",
    "plt.imshow(images[0][0],'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BtzKpf32uy3Z",
    "outputId": "f48bbbb7-be1c-4663-b1b5-cfde4ac17b5f"
   },
   "outputs": [],
   "source": [
    "images.resize_(images.shape[0], 1, 784)\n",
    "score = model(images[0,:])\n",
    "prob = nn.functional.softmax(score[0], dim=0)\n",
    "y_pred =  prob.argmax()\n",
    "print(\"Predicted class {} with probability {}\".format(y_pred, prob[y_pred]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
